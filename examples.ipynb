{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold, GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import optuna.integration\n",
    "from optuna.integration import OptunaSearchCV\n",
    "import optuna.integration.lightgbm as lgb_optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_split_train(file):\n",
    "    df = pd.read_csv(\"data.csv\", sep=';')\n",
    "    X, y = df.drop([\"ID\",\"TARGET\"], axis=1), df[\"TARGET\"]\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.1, random_state=8,stratify=y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.15, random_state=8,stratify=y_temp)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = read_data_split_train(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier(random_state=8, importance_type=\"gain\")\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM + Optuna (naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # define hyperparameters to be tuned\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 100),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 0.1),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 1000),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"subsample\": trial.suggest_uniform(\"subsample\", 0.1, 1),\n",
    "        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 1),\n",
    "        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-5, 10),\n",
    "        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-5, 10),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "    }\n",
    "\n",
    "    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"auc\")\n",
    "\n",
    "    dtrain = lgb_optuna.Dataset(X_train, y_train)\n",
    "    dval = lgb_optuna.Dataset(X_val, y_val)\n",
    "\n",
    "    # define the model\n",
    "    model = lgb_optuna.train(params,dtrain,valid_sets=[dval],\n",
    "                             callbacks=[pruning_callback],\n",
    "                            )\n",
    "\n",
    "    # evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    # return the objective value to be minimized\n",
    "    return 1 - auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"minimize\"\n",
    "    )\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OptunaSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9782/632472101.py:2: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.\n",
      "  opt = OptunaSearchCV(clf, param_distributions={\"max_depth\": optuna.distributions.IntDistribution(2, 4),\n",
      "[I 2023-07-17 01:22:06,191] A new study created in memory with name: no-name-578e0dd5-f67f-42e5-87e5-db70d94ebe0b\n",
      "[I 2023-07-17 01:22:06,192] Searching the best hyperparameters using 22950 samples...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\tvalid_0's auc: 0.723967\tvalid_0's binary_logloss: 0.510925\tvalid_1's auc: 0.72428\tvalid_1's binary_logloss: 0.511209\n",
      "[10]\tvalid_0's auc: 0.761013\tvalid_0's binary_logloss: 0.463964\tvalid_1's auc: 0.751909\tvalid_1's binary_logloss: 0.465758\n",
      "[20]\tvalid_0's auc: 0.759686\tvalid_0's binary_logloss: 0.497595\tvalid_1's auc: 0.751923\tvalid_1's binary_logloss: 0.498058\n",
      "[20]\tvalid_0's auc: 0.768284\tvalid_0's binary_logloss: 0.446395\tvalid_1's auc: 0.758571\tvalid_1's binary_logloss: 0.44903\n",
      "[30]\tvalid_0's auc: 0.75812\tvalid_0's binary_logloss: 0.48726\tvalid_1's auc: 0.751056\tvalid_1's binary_logloss: 0.487968\n",
      "[30]\tvalid_0's auc: 0.772833\tvalid_0's binary_logloss: 0.438643\tvalid_1's auc: 0.763319\tvalid_1's binary_logloss: 0.441585\n",
      "[40]\tvalid_0's auc: 0.761098\tvalid_0's binary_logloss: 0.479031\tvalid_1's auc: 0.751509\tvalid_1's binary_logloss: 0.480065\n",
      "[40]\tvalid_0's auc: 0.774956\tvalid_0's binary_logloss: 0.434411\tvalid_1's auc: 0.76468\tvalid_1's binary_logloss: 0.438098\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's auc: 0.774686\tvalid_0's binary_logloss: 0.435079\tvalid_1's auc: 0.764976\tvalid_1's binary_logloss: 0.438613\n",
      "[50]\tvalid_0's auc: 0.777201\tvalid_0's binary_logloss: 0.431918\tvalid_1's auc: 0.767081\tvalid_1's binary_logloss: 0.436048\n",
      "[60]\tvalid_0's auc: 0.779364\tvalid_0's binary_logloss: 0.430127\tvalid_1's auc: 0.769446\tvalid_1's binary_logloss: 0.43479\n",
      "[10]\tvalid_0's auc: 0.694802\tvalid_0's binary_logloss: 0.510962\tvalid_1's auc: 0.691968\tvalid_1's binary_logloss: 0.511152\n",
      "[70]\tvalid_0's auc: 0.781382\tvalid_0's binary_logloss: 0.428786\tvalid_1's auc: 0.771118\tvalid_1's binary_logloss: 0.43352\n",
      "[20]\tvalid_0's auc: 0.747079\tvalid_0's binary_logloss: 0.497576\tvalid_1's auc: 0.745581\tvalid_1's binary_logloss: 0.497892\n",
      "[80]\tvalid_0's auc: 0.782794\tvalid_0's binary_logloss: 0.427732\tvalid_1's auc: 0.772522\tvalid_1's binary_logloss: 0.43264\n",
      "[30]\tvalid_0's auc: 0.751637\tvalid_0's binary_logloss: 0.487182\tvalid_1's auc: 0.748843\tvalid_1's binary_logloss: 0.487601\n",
      "[90]\tvalid_0's auc: 0.784223\tvalid_0's binary_logloss: 0.426884\tvalid_1's auc: 0.774017\tvalid_1's binary_logloss: 0.431821\n",
      "[40]\tvalid_0's auc: 0.758284\tvalid_0's binary_logloss: 0.478946\tvalid_1's auc: 0.751647\tvalid_1's binary_logloss: 0.479592\n",
      "[50]\tvalid_0's auc: 0.758878\tvalid_0's binary_logloss: 0.472285\tvalid_1's auc: 0.751648\tvalid_1's binary_logloss: 0.473261\n",
      "[100]\tvalid_0's auc: 0.785395\tvalid_0's binary_logloss: 0.425961\tvalid_1's auc: 0.775211\tvalid_1's binary_logloss: 0.4312\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.785395\tvalid_0's binary_logloss: 0.425961\tvalid_1's auc: 0.775211\tvalid_1's binary_logloss: 0.4312\n",
      "[60]\tvalid_0's auc: 0.762401\tvalid_0's binary_logloss: 0.466842\tvalid_1's auc: 0.755664\tvalid_1's binary_logloss: 0.468132\n",
      "[70]\tvalid_0's auc: 0.763572\tvalid_0's binary_logloss: 0.462353\tvalid_1's auc: 0.755425\tvalid_1's binary_logloss: 0.463952\n",
      "[80]\tvalid_0's auc: 0.764437\tvalid_0's binary_logloss: 0.458622\tvalid_1's auc: 0.755555\tvalid_1's binary_logloss: 0.460532\n",
      "[90]\tvalid_0's auc: 0.76654\tvalid_0's binary_logloss: 0.455407\tvalid_1's auc: 0.757307\tvalid_1's binary_logloss: 0.457363\n",
      "[10]\tvalid_0's auc: 0.759532\tvalid_0's binary_logloss: 0.463891\tvalid_1's auc: 0.751402\tvalid_1's binary_logloss: 0.465373\n",
      "[100]\tvalid_0's auc: 0.767572\tvalid_0's binary_logloss: 0.452694\tvalid_1's auc: 0.757159\tvalid_1's binary_logloss: 0.454724\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.785395\tvalid_0's binary_logloss: 0.425961\tvalid_1's auc: 0.775211\tvalid_1's binary_logloss: 0.4312\n",
      "[20]\tvalid_0's auc: 0.768263\tvalid_0's binary_logloss: 0.446271\tvalid_1's auc: 0.757954\tvalid_1's binary_logloss: 0.448589\n",
      "[30]\tvalid_0's auc: 0.773193\tvalid_0's binary_logloss: 0.43825\tvalid_1's auc: 0.762185\tvalid_1's binary_logloss: 0.441172\n",
      "[40]\tvalid_0's auc: 0.774808\tvalid_0's binary_logloss: 0.434094\tvalid_1's auc: 0.764902\tvalid_1's binary_logloss: 0.437438\n",
      "[10]\tvalid_0's auc: 0.694798\tvalid_0's binary_logloss: 0.511093\tvalid_1's auc: 0.691777\tvalid_1's binary_logloss: 0.511433\n",
      "[20]\tvalid_0's auc: 0.749252\tvalid_0's binary_logloss: 0.497934\tvalid_1's auc: 0.73893\tvalid_1's binary_logloss: 0.498523\n",
      "[50]\tvalid_0's auc: 0.777197\tvalid_0's binary_logloss: 0.431742\tvalid_1's auc: 0.766402\tvalid_1's binary_logloss: 0.435626\n",
      "[30]\tvalid_0's auc: 0.751389\tvalid_0's binary_logloss: 0.48755\tvalid_1's auc: 0.742402\tvalid_1's binary_logloss: 0.488386\n",
      "[60]\tvalid_0's auc: 0.779743\tvalid_0's binary_logloss: 0.430066\tvalid_1's auc: 0.770234\tvalid_1's binary_logloss: 0.433911\n",
      "[40]\tvalid_0's auc: 0.752144\tvalid_0's binary_logloss: 0.479321\tvalid_1's auc: 0.74418\tvalid_1's binary_logloss: 0.480393\n",
      "[70]\tvalid_0's auc: 0.781862\tvalid_0's binary_logloss: 0.428688\tvalid_1's auc: 0.771785\tvalid_1's binary_logloss: 0.432805\n",
      "[50]\tvalid_0's auc: 0.756061\tvalid_0's binary_logloss: 0.472648\tvalid_1's auc: 0.749581\tvalid_1's binary_logloss: 0.47394\n",
      "[80]\tvalid_0's auc: 0.78338\tvalid_0's binary_logloss: 0.427439\tvalid_1's auc: 0.773171\tvalid_1's binary_logloss: 0.431953\n",
      "[60]\tvalid_0's auc: 0.756877\tvalid_0's binary_logloss: 0.467193\tvalid_1's auc: 0.749859\tvalid_1's binary_logloss: 0.468724\n",
      "[90]\tvalid_0's auc: 0.784619\tvalid_0's binary_logloss: 0.426508\tvalid_1's auc: 0.774198\tvalid_1's binary_logloss: 0.431363\n",
      "[70]\tvalid_0's auc: 0.759842\tvalid_0's binary_logloss: 0.4627\tvalid_1's auc: 0.751038\tvalid_1's binary_logloss: 0.464433\n",
      "[100]\tvalid_0's auc: 0.78586\tvalid_0's binary_logloss: 0.425709\tvalid_1's auc: 0.775249\tvalid_1's binary_logloss: 0.430857\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.78586\tvalid_0's binary_logloss: 0.425709\tvalid_1's auc: 0.775249\tvalid_1's binary_logloss: 0.430857\n",
      "[80]\tvalid_0's auc: 0.76119\tvalid_0's binary_logloss: 0.45892\tvalid_1's auc: 0.752565\tvalid_1's binary_logloss: 0.460668\n",
      "[90]\tvalid_0's auc: 0.76198\tvalid_0's binary_logloss: 0.455722\tvalid_1's auc: 0.754011\tvalid_1's binary_logloss: 0.457294\n",
      "[100]\tvalid_0's auc: 0.76567\tvalid_0's binary_logloss: 0.453003\tvalid_1's auc: 0.757802\tvalid_1's binary_logloss: 0.454548\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.78586\tvalid_0's binary_logloss: 0.425709\tvalid_1's auc: 0.775249\tvalid_1's binary_logloss: 0.430857\n",
      "[10]\tvalid_0's auc: 0.757234\tvalid_0's binary_logloss: 0.464259\tvalid_1's auc: 0.74902\tvalid_1's binary_logloss: 0.46593\n",
      "[10]\tvalid_0's auc: 0.694933\tvalid_0's binary_logloss: 0.510913\tvalid_1's auc: 0.691868\tvalid_1's binary_logloss: 0.511144\n",
      "[20]\tvalid_0's auc: 0.767801\tvalid_0's binary_logloss: 0.446554\tvalid_1's auc: 0.760583\tvalid_1's binary_logloss: 0.448155\n",
      "[20]\tvalid_0's auc: 0.756175\tvalid_0's binary_logloss: 0.497634\tvalid_1's auc: 0.75088\tvalid_1's binary_logloss: 0.497994\n",
      "[30]\tvalid_0's auc: 0.772696\tvalid_0's binary_logloss: 0.438742\tvalid_1's auc: 0.764744\tvalid_1's binary_logloss: 0.440704\n",
      "[30]\tvalid_0's auc: 0.756215\tvalid_0's binary_logloss: 0.487285\tvalid_1's auc: 0.750699\tvalid_1's binary_logloss: 0.487806\n",
      "[40]\tvalid_0's auc: 0.775555\tvalid_0's binary_logloss: 0.434403\tvalid_1's auc: 0.766382\tvalid_1's binary_logloss: 0.437274\n",
      "[40]\tvalid_0's auc: 0.762122\tvalid_0's binary_logloss: 0.479096\tvalid_1's auc: 0.757884\tvalid_1's binary_logloss: 0.479848\n",
      "[50]\tvalid_0's auc: 0.778571\tvalid_0's binary_logloss: 0.431825\tvalid_1's auc: 0.768722\tvalid_1's binary_logloss: 0.435378\n",
      "[50]\tvalid_0's auc: 0.763867\tvalid_0's binary_logloss: 0.472469\tvalid_1's auc: 0.758186\tvalid_1's binary_logloss: 0.473497\n",
      "[60]\tvalid_0's auc: 0.780411\tvalid_0's binary_logloss: 0.430124\tvalid_1's auc: 0.769858\tvalid_1's binary_logloss: 0.434025\n",
      "[60]\tvalid_0's auc: 0.763997\tvalid_0's binary_logloss: 0.467051\tvalid_1's auc: 0.758287\tvalid_1's binary_logloss: 0.468319\n",
      "[70]\tvalid_0's auc: 0.781981\tvalid_0's binary_logloss: 0.428754\tvalid_1's auc: 0.771885\tvalid_1's binary_logloss: 0.432862\n",
      "[70]\tvalid_0's auc: 0.765682\tvalid_0's binary_logloss: 0.462583\tvalid_1's auc: 0.758415\tvalid_1's binary_logloss: 0.464147\n",
      "[80]\tvalid_0's auc: 0.783328\tvalid_0's binary_logloss: 0.427647\tvalid_1's auc: 0.773474\tvalid_1's binary_logloss: 0.431994\n",
      "[80]\tvalid_0's auc: 0.767001\tvalid_0's binary_logloss: 0.458876\tvalid_1's auc: 0.760334\tvalid_1's binary_logloss: 0.460653\n",
      "[90]\tvalid_0's auc: 0.767638\tvalid_0's binary_logloss: 0.455717\tvalid_1's auc: 0.761125\tvalid_1's binary_logloss: 0.457573\n",
      "[90]\tvalid_0's auc: 0.784629\tvalid_0's binary_logloss: 0.426646\tvalid_1's auc: 0.774473\tvalid_1's binary_logloss: 0.431277\n",
      "[100]\tvalid_0's auc: 0.768468\tvalid_0's binary_logloss: 0.45304\tvalid_1's auc: 0.762118\tvalid_1's binary_logloss: 0.45489\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.78586\tvalid_0's binary_logloss: 0.425709\tvalid_1's auc: 0.775249\tvalid_1's binary_logloss: 0.430857\n",
      "[100]\tvalid_0's auc: 0.785847\tvalid_0's binary_logloss: 0.425815\tvalid_1's auc: 0.775098\tvalid_1's binary_logloss: 0.430793\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.78586\tvalid_0's binary_logloss: 0.425709\tvalid_1's auc: 0.775249\tvalid_1's binary_logloss: 0.430857\n",
      "[10]\tvalid_0's auc: 0.694764\tvalid_0's binary_logloss: 0.510891\tvalid_1's auc: 0.691544\tvalid_1's binary_logloss: 0.51138\n",
      "[20]\tvalid_0's auc: 0.751591\tvalid_0's binary_logloss: 0.497571\tvalid_1's auc: 0.750399\tvalid_1's binary_logloss: 0.498193\n",
      "[10]\tvalid_0's auc: 0.764501\tvalid_0's binary_logloss: 0.464059\tvalid_1's auc: 0.758395\tvalid_1's binary_logloss: 0.465462\n",
      "[30]\tvalid_0's auc: 0.751625\tvalid_0's binary_logloss: 0.487177\tvalid_1's auc: 0.750319\tvalid_1's binary_logloss: 0.487953\n",
      "[20]\tvalid_0's auc: 0.769327\tvalid_0's binary_logloss: 0.446605\tvalid_1's auc: 0.762176\tvalid_1's binary_logloss: 0.448673\n",
      "[40]\tvalid_0's auc: 0.755543\tvalid_0's binary_logloss: 0.478916\tvalid_1's auc: 0.751134\tvalid_1's binary_logloss: 0.479949\n",
      "[30]\tvalid_0's auc: 0.773403\tvalid_0's binary_logloss: 0.438947\tvalid_1's auc: 0.766827\tvalid_1's binary_logloss: 0.441063\n",
      "[50]\tvalid_0's auc: 0.758547\tvalid_0's binary_logloss: 0.472237\tvalid_1's auc: 0.750752\tvalid_1's binary_logloss: 0.473512\n",
      "[40]\tvalid_0's auc: 0.775385\tvalid_0's binary_logloss: 0.434662\tvalid_1's auc: 0.766506\tvalid_1's binary_logloss: 0.437822\n",
      "[60]\tvalid_0's auc: 0.758799\tvalid_0's binary_logloss: 0.466789\tvalid_1's auc: 0.750891\tvalid_1's binary_logloss: 0.46834\n",
      "[50]\tvalid_0's auc: 0.778054\tvalid_0's binary_logloss: 0.432179\tvalid_1's auc: 0.76813\tvalid_1's binary_logloss: 0.435695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-17 01:22:08,649] Trial 1 finished with value: 0.7617711310760507 and parameters: {'max_depth': 2, 'learning_rate': 0.010822912485045274}. Best is trial 1 with value: 0.7617711310760507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70]\tvalid_0's auc: 0.761024\tvalid_0's binary_logloss: 0.462299\tvalid_1's auc: 0.751805\tvalid_1's binary_logloss: 0.464099\n",
      "[80]\tvalid_0's auc: 0.762489\tvalid_0's binary_logloss: 0.458566\tvalid_1's auc: 0.752179\tvalid_1's binary_logloss: 0.460609\n",
      "[60]\tvalid_0's auc: 0.780142\tvalid_0's binary_logloss: 0.430508\tvalid_1's auc: 0.771529\tvalid_1's binary_logloss: 0.434212\n",
      "[90]\tvalid_0's auc: 0.7633\tvalid_0's binary_logloss: 0.455412\tvalid_1's auc: 0.753518\tvalid_1's binary_logloss: 0.457686\n",
      "[70]\tvalid_0's auc: 0.782148\tvalid_0's binary_logloss: 0.429142\tvalid_1's auc: 0.773488\tvalid_1's binary_logloss: 0.43302\n",
      "Early stopping, best iteration is:\n",
      "[75]\tvalid_0's auc: 0.783292\tvalid_0's binary_logloss: 0.428537\tvalid_1's auc: 0.775271\tvalid_1's binary_logloss: 0.432347\n",
      "[80]\tvalid_0's auc: 0.784374\tvalid_0's binary_logloss: 0.427841\tvalid_1's auc: 0.77612\tvalid_1's binary_logloss: 0.431817\n",
      "[90]\tvalid_0's auc: 0.785849\tvalid_0's binary_logloss: 0.426843\tvalid_1's auc: 0.77702\tvalid_1's binary_logloss: 0.431162\n",
      "[100]\tvalid_0's auc: 0.786916\tvalid_0's binary_logloss: 0.42596\tvalid_1's auc: 0.777746\tvalid_1's binary_logloss: 0.430576\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.786916\tvalid_0's binary_logloss: 0.42596\tvalid_1's auc: 0.777746\tvalid_1's binary_logloss: 0.430576\n",
      "[10]\tvalid_0's auc: 0.758118\tvalid_0's binary_logloss: 0.463843\tvalid_1's auc: 0.751924\tvalid_1's binary_logloss: 0.465645\n",
      "[20]\tvalid_0's auc: 0.766096\tvalid_0's binary_logloss: 0.446277\tvalid_1's auc: 0.755003\tvalid_1's binary_logloss: 0.449214\n",
      "[30]\tvalid_0's auc: 0.771969\tvalid_0's binary_logloss: 0.438565\tvalid_1's auc: 0.760328\tvalid_1's binary_logloss: 0.442206\n",
      "[40]\tvalid_0's auc: 0.775097\tvalid_0's binary_logloss: 0.434243\tvalid_1's auc: 0.764067\tvalid_1's binary_logloss: 0.438541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-17 01:22:09,141] Trial 0 finished with value: 0.7779272654323757 and parameters: {'max_depth': 2, 'learning_rate': 0.06908179863517087}. Best is trial 0 with value: 0.7779272654323757.\n",
      "[I 2023-07-17 01:22:09,142] Finished hyperparemeter search!\n",
      "[I 2023-07-17 01:22:09,143] Refitting the estimator using 22950 samples...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's auc: 0.778213\tvalid_0's binary_logloss: 0.431683\tvalid_1's auc: 0.769206\tvalid_1's binary_logloss: 0.436209\n",
      "[60]\tvalid_0's auc: 0.780018\tvalid_0's binary_logloss: 0.429909\tvalid_1's auc: 0.771289\tvalid_1's binary_logloss: 0.434566\n",
      "[70]\tvalid_0's auc: 0.781148\tvalid_0's binary_logloss: 0.428671\tvalid_1's auc: 0.772039\tvalid_1's binary_logloss: 0.43357\n",
      "[80]\tvalid_0's auc: 0.782696\tvalid_0's binary_logloss: 0.427596\tvalid_1's auc: 0.773173\tvalid_1's binary_logloss: 0.432621\n",
      "[90]\tvalid_0's auc: 0.784357\tvalid_0's binary_logloss: 0.426593\tvalid_1's auc: 0.774926\tvalid_1's binary_logloss: 0.431622\n",
      "[100]\tvalid_0's auc: 0.78575\tvalid_0's binary_logloss: 0.42566\tvalid_1's auc: 0.776284\tvalid_1's binary_logloss: 0.430721\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.786916\tvalid_0's binary_logloss: 0.42596\tvalid_1's auc: 0.777746\tvalid_1's binary_logloss: 0.430576\n",
      "[10]\ttraining's auc: 0.75941\ttraining's binary_logloss: 0.463981\tvalid_1's auc: 0.752397\tvalid_1's binary_logloss: 0.465518\n",
      "[20]\ttraining's auc: 0.768624\ttraining's binary_logloss: 0.446378\tvalid_1's auc: 0.758649\tvalid_1's binary_logloss: 0.448649\n",
      "[30]\ttraining's auc: 0.773212\ttraining's binary_logloss: 0.43851\tvalid_1's auc: 0.762968\tvalid_1's binary_logloss: 0.441401\n",
      "[40]\ttraining's auc: 0.775418\ttraining's binary_logloss: 0.434277\tvalid_1's auc: 0.764927\tvalid_1's binary_logloss: 0.437774\n",
      "[50]\ttraining's auc: 0.778052\ttraining's binary_logloss: 0.431796\tvalid_1's auc: 0.766652\tvalid_1's binary_logloss: 0.435844\n",
      "[60]\ttraining's auc: 0.78026\ttraining's binary_logloss: 0.430082\tvalid_1's auc: 0.76892\tvalid_1's binary_logloss: 0.434397\n",
      "[70]\ttraining's auc: 0.782499\ttraining's binary_logloss: 0.428619\tvalid_1's auc: 0.771907\tvalid_1's binary_logloss: 0.433179\n",
      "[80]\ttraining's auc: 0.784115\ttraining's binary_logloss: 0.427485\tvalid_1's auc: 0.773071\tvalid_1's binary_logloss: 0.432243\n",
      "[90]\ttraining's auc: 0.785397\ttraining's binary_logloss: 0.426356\tvalid_1's auc: 0.774332\tvalid_1's binary_logloss: 0.431407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-17 01:22:09,438] Finished refitting! (elapsed time: 0.294 sec.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's auc: 0.786582\ttraining's binary_logloss: 0.425477\tvalid_1's auc: 0.774874\tvalid_1's binary_logloss: 0.431061\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's auc: 0.786916\tvalid_0's binary_logloss: 0.42596\tvalid_1's auc: 0.777746\tvalid_1's binary_logloss: 0.430576\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OptunaSearchCV(cv=KFold(n_splits=5, random_state=8, shuffle=True),\n",
       "               estimator=LGBMClassifier(importance_type=&#x27;gain&#x27;, random_state=8),\n",
       "               n_jobs=-1, n_trials=2,\n",
       "               param_distributions={&#x27;learning_rate&#x27;: FloatDistribution(high=0.1, log=True, low=0.001, step=None),\n",
       "                                    &#x27;max_depth&#x27;: IntDistribution(high=4, log=False, low=2, step=1)},\n",
       "               random_state=8, return_train_score=True, scoring=&#x27;roc_auc&#x27;,\n",
       "               verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OptunaSearchCV</label><div class=\"sk-toggleable__content\"><pre>OptunaSearchCV(cv=KFold(n_splits=5, random_state=8, shuffle=True),\n",
       "               estimator=LGBMClassifier(importance_type=&#x27;gain&#x27;, random_state=8),\n",
       "               n_jobs=-1, n_trials=2,\n",
       "               param_distributions={&#x27;learning_rate&#x27;: FloatDistribution(high=0.1, log=True, low=0.001, step=None),\n",
       "                                    &#x27;max_depth&#x27;: IntDistribution(high=4, log=False, low=2, step=1)},\n",
       "               random_state=8, return_train_score=True, scoring=&#x27;roc_auc&#x27;,\n",
       "               verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(importance_type=&#x27;gain&#x27;, random_state=8)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(importance_type=&#x27;gain&#x27;, random_state=8)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "OptunaSearchCV(cv=KFold(n_splits=5, random_state=8, shuffle=True),\n",
       "               estimator=LGBMClassifier(importance_type='gain', random_state=8),\n",
       "               n_jobs=-1, n_trials=2,\n",
       "               param_distributions={'learning_rate': FloatDistribution(high=0.1, log=True, low=0.001, step=None),\n",
       "                                    'max_depth': IntDistribution(high=4, log=False, low=2, step=1)},\n",
       "               random_state=8, return_train_score=True, scoring='roc_auc',\n",
       "               verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results={}\n",
    "opt = OptunaSearchCV(clf, param_distributions={\"max_depth\": optuna.distributions.IntDistribution(2, 4),\n",
    "                                               \"learning_rate\": optuna.distributions.FloatDistribution(high=0.1, log=True, low=0.001),},\n",
    "                                               n_trials=2, verbose=1, random_state=8, n_jobs=-1, cv=folds, scoring=\"roc_auc\", return_train_score=True)\n",
    "\n",
    "opt.fit(X_train, y_train,eval_set=[(X_train,y_train),(X_val, y_val)], eval_metric=\"auc\",callbacks=[lgb.early_stopping(10),lgb.log_evaluation(period=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBMTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters to be tuned\n",
    "params = {\"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        # \"verbosity\": 1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"n_jobs\": -1\n",
    "}\n",
    "dtrain = lgb_optuna.Dataset(X_train, y_train)\n",
    "dval = lgb_optuna.Dataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-17 02:03:41,913] A new study created in memory with name: no-name-030cb193-949a-4ce0-8e35-4870be4c0be6\n",
      "feature_fraction, val_score: 0.425302:  14%|#4        | 1/7 [00:00<00:01,  4.56it/s][I 2023-07-17 02:03:42,138] Trial 0 finished with value: 0.42530226908388774 and parameters: {'feature_fraction': 0.7}. Best is trial 0 with value: 0.42530226908388774.\n",
      "feature_fraction, val_score: 0.425302:  14%|#4        | 1/7 [00:00<00:01,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003023 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[10]\tvalid_0's binary_logloss: 0.443873\tvalid_1's binary_logloss: 0.449979\n",
      "[20]\tvalid_0's binary_logloss: 0.418129\tvalid_1's binary_logloss: 0.431118\n",
      "[30]\tvalid_0's binary_logloss: 0.404493\tvalid_1's binary_logloss: 0.425965\n",
      "[40]\tvalid_0's binary_logloss: 0.395246\tvalid_1's binary_logloss: 0.425422\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's binary_logloss: 0.396877\tvalid_1's binary_logloss: 0.425302\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000861 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.434941\tvalid_1's binary_logloss: 0.442945\n",
      "[20]\tvalid_0's binary_logloss: 0.413178\tvalid_1's binary_logloss: 0.429099\n",
      "[30]\tvalid_0's binary_logloss: 0.40147\tvalid_1's binary_logloss: 0.42591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.425302:  29%|##8       | 2/7 [00:00<00:01,  3.48it/s][I 2023-07-17 02:03:42,474] Trial 1 finished with value: 0.42530226908388774 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 0 with value: 0.42530226908388774.\n",
      "feature_fraction, val_score: 0.425302:  29%|##8       | 2/7 [00:00<00:01,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]\tvalid_0's binary_logloss: 0.392585\tvalid_1's binary_logloss: 0.426052\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's binary_logloss: 0.396877\tvalid_1's binary_logloss: 0.425302\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000389 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.453317\tvalid_1's binary_logloss: 0.459751\n",
      "[20]\tvalid_0's binary_logloss: 0.424176\tvalid_1's binary_logloss: 0.437212\n",
      "[30]\tvalid_0's binary_logloss: 0.409987\tvalid_1's binary_logloss: 0.429731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.425302:  43%|####2     | 3/7 [00:00<00:01,  3.98it/s][I 2023-07-17 02:03:42,682] Trial 2 finished with value: 0.42530226908388774 and parameters: {'feature_fraction': 0.4}. Best is trial 0 with value: 0.42530226908388774.\n",
      "feature_fraction, val_score: 0.425302:  43%|####2     | 3/7 [00:00<00:01,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]\tvalid_0's binary_logloss: 0.400513\tvalid_1's binary_logloss: 0.426742\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid_0's binary_logloss: 0.396877\tvalid_1's binary_logloss: 0.425302\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000798 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.441231\tvalid_1's binary_logloss: 0.447832\n",
      "[20]\tvalid_0's binary_logloss: 0.416449\tvalid_1's binary_logloss: 0.4302\n",
      "[30]\tvalid_0's binary_logloss: 0.403423\tvalid_1's binary_logloss: 0.426256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.424381:  57%|#####7    | 4/7 [00:01<00:00,  4.03it/s][I 2023-07-17 02:03:42,924] Trial 3 finished with value: 0.4243808317096525 and parameters: {'feature_fraction': 0.8}. Best is trial 3 with value: 0.4243808317096525.\n",
      "feature_fraction, val_score: 0.424381:  57%|#####7    | 4/7 [00:01<00:00,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]\tvalid_0's binary_logloss: 0.394397\tvalid_1's binary_logloss: 0.424498\n",
      "[50]\tvalid_0's binary_logloss: 0.386766\tvalid_1's binary_logloss: 0.424609\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000585 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.444252\tvalid_1's binary_logloss: 0.450845\n",
      "[20]\tvalid_0's binary_logloss: 0.41838\tvalid_1's binary_logloss: 0.432378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.424381:  71%|#######1  | 5/7 [00:01<00:00,  3.93it/s][I 2023-07-17 02:03:43,192] Trial 4 finished with value: 0.4243808317096525 and parameters: {'feature_fraction': 0.6}. Best is trial 3 with value: 0.4243808317096525.\n",
      "feature_fraction, val_score: 0.424381:  71%|#######1  | 5/7 [00:01<00:00,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30]\tvalid_0's binary_logloss: 0.404681\tvalid_1's binary_logloss: 0.426907\n",
      "[40]\tvalid_0's binary_logloss: 0.395563\tvalid_1's binary_logloss: 0.425432\n",
      "[50]\tvalid_0's binary_logloss: 0.388585\tvalid_1's binary_logloss: 0.424926\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000480 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.445096\tvalid_1's binary_logloss: 0.451957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.424381:  86%|########5 | 6/7 [00:01<00:00,  4.00it/s][I 2023-07-17 02:03:43,432] Trial 5 finished with value: 0.4243808317096525 and parameters: {'feature_fraction': 0.5}. Best is trial 3 with value: 0.4243808317096525.\n",
      "feature_fraction, val_score: 0.424381:  86%|########5 | 6/7 [00:01<00:00,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\tvalid_0's binary_logloss: 0.420025\tvalid_1's binary_logloss: 0.433398\n",
      "[30]\tvalid_0's binary_logloss: 0.407171\tvalid_1's binary_logloss: 0.428236\n",
      "[40]\tvalid_0's binary_logloss: 0.397367\tvalid_1's binary_logloss: 0.425241\n",
      "[50]\tvalid_0's binary_logloss: 0.390376\tvalid_1's binary_logloss: 0.424899\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000847 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.434413\tvalid_1's binary_logloss: 0.444102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.424381: 100%|##########| 7/7 [00:01<00:00,  4.06it/s][I 2023-07-17 02:03:43,671] Trial 6 finished with value: 0.4243808317096525 and parameters: {'feature_fraction': 1.0}. Best is trial 3 with value: 0.4243808317096525.\n",
      "feature_fraction, val_score: 0.424381: 100%|##########| 7/7 [00:01<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\tvalid_0's binary_logloss: 0.413501\tvalid_1's binary_logloss: 0.431994\n",
      "[30]\tvalid_0's binary_logloss: 0.402089\tvalid_1's binary_logloss: 0.428933\n",
      "[40]\tvalid_0's binary_logloss: 0.392961\tvalid_1's binary_logloss: 0.428154\n",
      "[50]\tvalid_0's binary_logloss: 0.385522\tvalid_1's binary_logloss: 0.428047\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.424381:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.415205\tvalid_1's binary_logloss: 0.447154\n",
      "[20]\tvalid_0's binary_logloss: 0.370779\tvalid_1's binary_logloss: 0.43161\n",
      "[30]\tvalid_0's binary_logloss: 0.339757\tvalid_1's binary_logloss: 0.429094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.424381:   5%|5         | 1/20 [00:00<00:11,  1.61it/s][I 2023-07-17 02:03:44,300] Trial 7 finished with value: 0.4243808317096525 and parameters: {'num_leaves': 129}. Best is trial 7 with value: 0.4243808317096525.\n",
      "num_leaves, val_score: 0.424381:   5%|5         | 1/20 [00:00<00:11,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]\tvalid_0's binary_logloss: 0.316225\tvalid_1's binary_logloss: 0.430458\n",
      "[50]\tvalid_0's binary_logloss: 0.295916\tvalid_1's binary_logloss: 0.432388\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000777 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.429873\tvalid_1's binary_logloss: 0.446604\n",
      "[20]\tvalid_0's binary_logloss: 0.396138\tvalid_1's binary_logloss: 0.430524\n",
      "[30]\tvalid_0's binary_logloss: 0.375498\tvalid_1's binary_logloss: 0.427049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.424381:  10%|#         | 2/20 [00:01<00:09,  1.95it/s][I 2023-07-17 02:03:44,734] Trial 8 finished with value: 0.4243808317096525 and parameters: {'num_leaves': 69}. Best is trial 7 with value: 0.4243808317096525.\n",
      "num_leaves, val_score: 0.424381:  10%|#         | 2/20 [00:01<00:09,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]\tvalid_0's binary_logloss: 0.359744\tvalid_1's binary_logloss: 0.427063\n",
      "[50]\tvalid_0's binary_logloss: 0.346542\tvalid_1's binary_logloss: 0.428404\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.425382\tvalid_1's binary_logloss: 0.446056\n",
      "[20]\tvalid_0's binary_logloss: 0.38818\tvalid_1's binary_logloss: 0.431248\n",
      "[30]\tvalid_0's binary_logloss: 0.364033\tvalid_1's binary_logloss: 0.427894\n",
      "[40]\tvalid_0's binary_logloss: 0.345106\tvalid_1's binary_logloss: 0.427854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.424381:  15%|#5        | 3/20 [00:01<00:07,  2.16it/s][I 2023-07-17 02:03:45,142] Trial 9 finished with value: 0.4243808317096525 and parameters: {'num_leaves': 87}. Best is trial 7 with value: 0.4243808317096525.\n",
      "num_leaves, val_score: 0.424381:  15%|#5        | 3/20 [00:01<00:07,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's binary_logloss: 0.329921\tvalid_1's binary_logloss: 0.42985\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.393714\tvalid_1's binary_logloss: 0.446628\n",
      "[20]\tvalid_0's binary_logloss: 0.33349\tvalid_1's binary_logloss: 0.434431\n",
      "[30]\tvalid_0's binary_logloss: 0.290878\tvalid_1's binary_logloss: 0.435098\n",
      "[40]\tvalid_0's binary_logloss: 0.257664\tvalid_1's binary_logloss: 0.437297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.424381:  20%|##        | 4/20 [00:02<00:08,  1.80it/s][I 2023-07-17 02:03:45,836] Trial 10 finished with value: 0.4243808317096525 and parameters: {'num_leaves': 241}. Best is trial 7 with value: 0.4243808317096525.\n",
      "num_leaves, val_score: 0.424381:  20%|##        | 4/20 [00:02<00:08,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's binary_logloss: 0.230988\tvalid_1's binary_logloss: 0.4405\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000773 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.391729\tvalid_1's binary_logloss: 0.446771\n",
      "[20]\tvalid_0's binary_logloss: 0.331021\tvalid_1's binary_logloss: 0.43264\n",
      "[30]\tvalid_0's binary_logloss: 0.287948\tvalid_1's binary_logloss: 0.433463\n",
      "[40]\tvalid_0's binary_logloss: 0.254072\tvalid_1's binary_logloss: 0.436499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.424381:  25%|##5       | 5/20 [00:03<00:10,  1.48it/s][I 2023-07-17 02:03:46,727] Trial 11 finished with value: 0.4243808317096525 and parameters: {'num_leaves': 249}. Best is trial 7 with value: 0.4243808317096525.\n",
      "num_leaves, val_score: 0.424381:  25%|##5       | 5/20 [00:03<00:10,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's binary_logloss: 0.227135\tvalid_1's binary_logloss: 0.440626\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.450791\tvalid_1's binary_logloss: 0.451929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.424381:  30%|###       | 6/20 [00:03<00:07,  1.97it/s][I 2023-07-17 02:03:46,904] Trial 12 finished with value: 0.4243808317096525 and parameters: {'num_leaves': 9}. Best is trial 7 with value: 0.4243808317096525.\n",
      "num_leaves, val_score: 0.424381:  30%|###       | 6/20 [00:03<00:07,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\tvalid_0's binary_logloss: 0.43144\tvalid_1's binary_logloss: 0.434293\n",
      "[30]\tvalid_0's binary_logloss: 0.423855\tvalid_1's binary_logloss: 0.428411\n",
      "[40]\tvalid_0's binary_logloss: 0.420137\tvalid_1's binary_logloss: 0.427089\n",
      "[50]\tvalid_0's binary_logloss: 0.41737\tvalid_1's binary_logloss: 0.426586\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000782 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.403487\tvalid_1's binary_logloss: 0.44676\n",
      "[20]\tvalid_0's binary_logloss: 0.3503\tvalid_1's binary_logloss: 0.431683\n",
      "[30]\tvalid_0's binary_logloss: 0.31303\tvalid_1's binary_logloss: 0.430888\n",
      "[40]\tvalid_0's binary_logloss: 0.283045\tvalid_1's binary_logloss: 0.43235\n",
      "[50]\tvalid_0's binary_logloss: 0.258721\tvalid_1's binary_logloss: 0.435262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.424381:  35%|###5      | 7/20 [00:04<00:07,  1.68it/s][I 2023-07-17 02:03:47,685] Trial 13 finished with value: 0.4243808317096525 and parameters: {'num_leaves': 185}. Best is trial 7 with value: 0.4243808317096525.\n",
      "num_leaves, val_score: 0.424381:  35%|###5      | 7/20 [00:04<00:07,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000798 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.445092\tvalid_1's binary_logloss: 0.449169\n",
      "[20]\tvalid_0's binary_logloss: 0.422793\tvalid_1's binary_logloss: 0.43174\n",
      "[30]\tvalid_0's binary_logloss: 0.412598\tvalid_1's binary_logloss: 0.426479\n",
      "[40]\tvalid_0's binary_logloss: 0.406058\tvalid_1's binary_logloss: 0.425688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.424381:  40%|####      | 8/20 [00:04<00:05,  2.10it/s][I 2023-07-17 02:03:47,901] Trial 14 finished with value: 0.4243808317096525 and parameters: {'num_leaves': 20}. Best is trial 7 with value: 0.4243808317096525.\n",
      "num_leaves, val_score: 0.424381:  40%|####      | 8/20 [00:04<00:05,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's binary_logloss: 0.400842\tvalid_1's binary_logloss: 0.425053\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000783 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.407126\tvalid_1's binary_logloss: 0.446389\n",
      "[20]\tvalid_0's binary_logloss: 0.356245\tvalid_1's binary_logloss: 0.430982\n",
      "[30]\tvalid_0's binary_logloss: 0.32112\tvalid_1's binary_logloss: 0.427858\n",
      "[40]\tvalid_0's binary_logloss: 0.292669\tvalid_1's binary_logloss: 0.429918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.424381:  45%|####5     | 9/20 [00:04<00:05,  2.02it/s][I 2023-07-17 02:03:48,437] Trial 15 finished with value: 0.4243808317096525 and parameters: {'num_leaves': 167}. Best is trial 7 with value: 0.4243808317096525.\n",
      "num_leaves, val_score: 0.424381:  45%|####5     | 9/20 [00:04<00:05,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's binary_logloss: 0.269807\tvalid_1's binary_logloss: 0.43205\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.403237\tvalid_1's binary_logloss: 0.446745\n",
      "[20]\tvalid_0's binary_logloss: 0.349826\tvalid_1's binary_logloss: 0.43165\n",
      "[30]\tvalid_0's binary_logloss: 0.312792\tvalid_1's binary_logloss: 0.431279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.424381:  50%|#####     | 10/20 [00:05<00:05,  1.78it/s][I 2023-07-17 02:03:49,153] Trial 16 finished with value: 0.4243808317096525 and parameters: {'num_leaves': 188}. Best is trial 7 with value: 0.4243808317096525.\n",
      "num_leaves, val_score: 0.424381:  50%|#####     | 10/20 [00:05<00:05,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]\tvalid_0's binary_logloss: 0.28362\tvalid_1's binary_logloss: 0.432292\n",
      "[50]\tvalid_0's binary_logloss: 0.259399\tvalid_1's binary_logloss: 0.435424\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007186 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.425037\tvalid_1's binary_logloss: 0.445796\n",
      "[20]\tvalid_0's binary_logloss: 0.387493\tvalid_1's binary_logloss: 0.429328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.424381:  55%|#####5    | 11/20 [00:05<00:04,  1.89it/s][I 2023-07-17 02:03:49,604] Trial 17 finished with value: 0.4243808317096525 and parameters: {'num_leaves': 88}. Best is trial 7 with value: 0.4243808317096525.\n",
      "num_leaves, val_score: 0.424381:  55%|#####5    | 11/20 [00:05<00:04,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30]\tvalid_0's binary_logloss: 0.363441\tvalid_1's binary_logloss: 0.426483\n",
      "[40]\tvalid_0's binary_logloss: 0.344476\tvalid_1's binary_logloss: 0.426372\n",
      "[50]\tvalid_0's binary_logloss: 0.328762\tvalid_1's binary_logloss: 0.427056\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000802 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.412\tvalid_1's binary_logloss: 0.447074\n",
      "[20]\tvalid_0's binary_logloss: 0.365485\tvalid_1's binary_logloss: 0.432192\n",
      "[30]\tvalid_0's binary_logloss: 0.333532\tvalid_1's binary_logloss: 0.430133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.424381:  60%|######    | 12/20 [00:06<00:04,  1.87it/s][I 2023-07-17 02:03:50,155] Trial 18 finished with value: 0.4243808317096525 and parameters: {'num_leaves': 142}. Best is trial 7 with value: 0.4243808317096525.\n",
      "num_leaves, val_score: 0.424381:  60%|######    | 12/20 [00:06<00:04,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]\tvalid_0's binary_logloss: 0.308237\tvalid_1's binary_logloss: 0.430736\n",
      "[50]\tvalid_0's binary_logloss: 0.287405\tvalid_1's binary_logloss: 0.43305\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000789 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.424381:  65%|######5   | 13/20 [00:06<00:03,  2.19it/s][I 2023-07-17 02:03:50,433] Trial 19 finished with value: 0.4243808317096525 and parameters: {'num_leaves': 43}. Best is trial 7 with value: 0.4243808317096525.\n",
      "num_leaves, val_score: 0.424381:  65%|######5   | 13/20 [00:06<00:03,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's binary_logloss: 0.437406\tvalid_1's binary_logloss: 0.446819\n",
      "[20]\tvalid_0's binary_logloss: 0.409529\tvalid_1's binary_logloss: 0.430373\n",
      "[30]\tvalid_0's binary_logloss: 0.393612\tvalid_1's binary_logloss: 0.426677\n",
      "[40]\tvalid_0's binary_logloss: 0.382028\tvalid_1's binary_logloss: 0.426163\n",
      "[50]\tvalid_0's binary_logloss: 0.372773\tvalid_1's binary_logloss: 0.42691\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.399555\tvalid_1's binary_logloss: 0.447264\n",
      "[20]\tvalid_0's binary_logloss: 0.34323\tvalid_1's binary_logloss: 0.43333\n",
      "[30]\tvalid_0's binary_logloss: 0.303691\tvalid_1's binary_logloss: 0.432759\n",
      "[40]\tvalid_0's binary_logloss: 0.273418\tvalid_1's binary_logloss: 0.435592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.424381:  70%|#######   | 14/20 [00:07<00:03,  1.87it/s][I 2023-07-17 02:03:51,144] Trial 20 finished with value: 0.4243808317096525 and parameters: {'num_leaves': 207}. Best is trial 7 with value: 0.4243808317096525.\n",
      "num_leaves, val_score: 0.424381:  70%|#######   | 14/20 [00:07<00:03,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's binary_logloss: 0.248345\tvalid_1's binary_logloss: 0.438224\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.41616\tvalid_1's binary_logloss: 0.445975\n",
      "[20]\tvalid_0's binary_logloss: 0.372028\tvalid_1's binary_logloss: 0.431368\n",
      "[30]\tvalid_0's binary_logloss: 0.341894\tvalid_1's binary_logloss: 0.429941\n",
      "[40]\tvalid_0's binary_logloss: 0.318345\tvalid_1's binary_logloss: 0.430322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.424381:  75%|#######5  | 15/20 [00:07<00:02,  1.96it/s][I 2023-07-17 02:03:51,594] Trial 21 finished with value: 0.4243808317096525 and parameters: {'num_leaves': 125}. Best is trial 7 with value: 0.4243808317096525.\n",
      "num_leaves, val_score: 0.424381:  75%|#######5  | 15/20 [00:07<00:02,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's binary_logloss: 0.299452\tvalid_1's binary_logloss: 0.433243\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000777 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.396159\tvalid_1's binary_logloss: 0.448495\n",
      "[20]\tvalid_0's binary_logloss: 0.337094\tvalid_1's binary_logloss: 0.433019\n",
      "[30]\tvalid_0's binary_logloss: 0.295333\tvalid_1's binary_logloss: 0.432052\n",
      "[40]\tvalid_0's binary_logloss: 0.263588\tvalid_1's binary_logloss: 0.435485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.424381:  80%|########  | 16/20 [00:08<00:02,  1.73it/s][I 2023-07-17 02:03:52,332] Trial 22 finished with value: 0.4243808317096525 and parameters: {'num_leaves': 228}. Best is trial 7 with value: 0.4243808317096525.\n",
      "num_leaves, val_score: 0.424381:  80%|########  | 16/20 [00:08<00:02,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's binary_logloss: 0.237936\tvalid_1's binary_logloss: 0.439077\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000762 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.435152\tvalid_1's binary_logloss: 0.446857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.424381:  85%|########5 | 17/20 [00:08<00:01,  2.05it/s][I 2023-07-17 02:03:52,612] Trial 23 finished with value: 0.4243808317096525 and parameters: {'num_leaves': 50}. Best is trial 7 with value: 0.4243808317096525.\n",
      "num_leaves, val_score: 0.424381:  85%|########5 | 17/20 [00:08<00:01,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\tvalid_0's binary_logloss: 0.405648\tvalid_1's binary_logloss: 0.430033\n",
      "[30]\tvalid_0's binary_logloss: 0.388627\tvalid_1's binary_logloss: 0.426873\n",
      "[40]\tvalid_0's binary_logloss: 0.376058\tvalid_1's binary_logloss: 0.426365\n",
      "[50]\tvalid_0's binary_logloss: 0.365093\tvalid_1's binary_logloss: 0.426788\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.422086\tvalid_1's binary_logloss: 0.446349\n",
      "[20]\tvalid_0's binary_logloss: 0.382369\tvalid_1's binary_logloss: 0.429376\n",
      "[30]\tvalid_0's binary_logloss: 0.35625\tvalid_1's binary_logloss: 0.426567\n",
      "[40]\tvalid_0's binary_logloss: 0.335687\tvalid_1's binary_logloss: 0.426967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.424381:  90%|######### | 18/20 [00:09<00:00,  2.18it/s][I 2023-07-17 02:03:53,002] Trial 24 finished with value: 0.4243808317096525 and parameters: {'num_leaves': 100}. Best is trial 7 with value: 0.4243808317096525.\n",
      "num_leaves, val_score: 0.424381:  90%|######### | 18/20 [00:09<00:00,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's binary_logloss: 0.319403\tvalid_1's binary_logloss: 0.428323\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000802 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.409888\tvalid_1's binary_logloss: 0.446404\n",
      "[20]\tvalid_0's binary_logloss: 0.360831\tvalid_1's binary_logloss: 0.431708\n",
      "[30]\tvalid_0's binary_logloss: 0.326252\tvalid_1's binary_logloss: 0.429584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.424381:  95%|#########5| 19/20 [00:09<00:00,  2.00it/s][I 2023-07-17 02:03:53,601] Trial 25 finished with value: 0.4243808317096525 and parameters: {'num_leaves': 155}. Best is trial 7 with value: 0.4243808317096525.\n",
      "num_leaves, val_score: 0.424381:  95%|#########5| 19/20 [00:09<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]\tvalid_0's binary_logloss: 0.299908\tvalid_1's binary_logloss: 0.431904\n",
      "[50]\tvalid_0's binary_logloss: 0.278145\tvalid_1's binary_logloss: 0.434639\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000784 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.419785\tvalid_1's binary_logloss: 0.446657\n",
      "[20]\tvalid_0's binary_logloss: 0.378583\tvalid_1's binary_logloss: 0.429473\n",
      "[30]\tvalid_0's binary_logloss: 0.350734\tvalid_1's binary_logloss: 0.426506\n",
      "[40]\tvalid_0's binary_logloss: 0.328202\tvalid_1's binary_logloss: 0.42799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.424381: 100%|##########| 20/20 [00:10<00:00,  2.09it/s][I 2023-07-17 02:03:54,027] Trial 26 finished with value: 0.4243808317096525 and parameters: {'num_leaves': 110}. Best is trial 7 with value: 0.4243808317096525.\n",
      "num_leaves, val_score: 0.424381: 100%|##########| 20/20 [00:10<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's binary_logloss: 0.310465\tvalid_1's binary_logloss: 0.430195\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.424381:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000776 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.441287\tvalid_1's binary_logloss: 0.448136\n",
      "[20]\tvalid_0's binary_logloss: 0.416259\tvalid_1's binary_logloss: 0.430388\n",
      "[30]\tvalid_0's binary_logloss: 0.403452\tvalid_1's binary_logloss: 0.426083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.424381:  10%|#         | 1/10 [00:00<00:03,  2.87it/s][I 2023-07-17 02:03:54,381] Trial 27 finished with value: 0.4243808317096525 and parameters: {'bagging_fraction': 0.6871685618170477, 'bagging_freq': 4}. Best is trial 27 with value: 0.4243808317096525.\n",
      "bagging, val_score: 0.424381:  10%|#         | 1/10 [00:00<00:03,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]\tvalid_0's binary_logloss: 0.39487\tvalid_1's binary_logloss: 0.425019\n",
      "[50]\tvalid_0's binary_logloss: 0.387249\tvalid_1's binary_logloss: 0.424395\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001243 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.441366\tvalid_1's binary_logloss: 0.447905\n",
      "[20]\tvalid_0's binary_logloss: 0.416584\tvalid_1's binary_logloss: 0.430277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.424381:  20%|##        | 2/10 [00:00<00:02,  2.68it/s][I 2023-07-17 02:03:54,772] Trial 28 finished with value: 0.4243808317096525 and parameters: {'bagging_fraction': 0.9839549828440374, 'bagging_freq': 1}. Best is trial 27 with value: 0.4243808317096525.\n",
      "bagging, val_score: 0.424381:  20%|##        | 2/10 [00:00<00:02,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30]\tvalid_0's binary_logloss: 0.403496\tvalid_1's binary_logloss: 0.425763\n",
      "[40]\tvalid_0's binary_logloss: 0.394306\tvalid_1's binary_logloss: 0.425102\n",
      "[50]\tvalid_0's binary_logloss: 0.387112\tvalid_1's binary_logloss: 0.425762\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002119 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.442515\tvalid_1's binary_logloss: 0.449035\n",
      "[20]\tvalid_0's binary_logloss: 0.417651\tvalid_1's binary_logloss: 0.431212\n",
      "[30]\tvalid_0's binary_logloss: 0.405071\tvalid_1's binary_logloss: 0.42728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.424381:  30%|###       | 3/10 [00:01<00:02,  2.89it/s][I 2023-07-17 02:03:55,084] Trial 29 finished with value: 0.4243808317096525 and parameters: {'bagging_fraction': 0.45532376713477135, 'bagging_freq': 7}. Best is trial 27 with value: 0.4243808317096525.\n",
      "bagging, val_score: 0.424381:  30%|###       | 3/10 [00:01<00:02,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]\tvalid_0's binary_logloss: 0.39724\tvalid_1's binary_logloss: 0.427037\n",
      "[50]\tvalid_0's binary_logloss: 0.390516\tvalid_1's binary_logloss: 0.42739\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000782 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.441359\tvalid_1's binary_logloss: 0.448166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.424381:  40%|####      | 4/10 [00:01<00:02,  2.81it/s][I 2023-07-17 02:03:55,455] Trial 30 finished with value: 0.4243808317096525 and parameters: {'bagging_fraction': 0.42984750921501924, 'bagging_freq': 1}. Best is trial 27 with value: 0.4243808317096525.\n",
      "bagging, val_score: 0.424381:  40%|####      | 4/10 [00:01<00:02,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\tvalid_0's binary_logloss: 0.416778\tvalid_1's binary_logloss: 0.431554\n",
      "[30]\tvalid_0's binary_logloss: 0.404852\tvalid_1's binary_logloss: 0.427286\n",
      "[40]\tvalid_0's binary_logloss: 0.396209\tvalid_1's binary_logloss: 0.427838\n",
      "[50]\tvalid_0's binary_logloss: 0.389181\tvalid_1's binary_logloss: 0.428683\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.424381:  50%|#####     | 5/10 [00:01<00:01,  3.04it/s][I 2023-07-17 02:03:55,737] Trial 31 finished with value: 0.4243808317096525 and parameters: {'bagging_fraction': 0.93567087121987, 'bagging_freq': 7}. Best is trial 27 with value: 0.4243808317096525.\n",
      "bagging, val_score: 0.424381:  50%|#####     | 5/10 [00:01<00:01,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's binary_logloss: 0.44137\tvalid_1's binary_logloss: 0.447864\n",
      "[20]\tvalid_0's binary_logloss: 0.416198\tvalid_1's binary_logloss: 0.430551\n",
      "[30]\tvalid_0's binary_logloss: 0.403017\tvalid_1's binary_logloss: 0.42617\n",
      "[40]\tvalid_0's binary_logloss: 0.39372\tvalid_1's binary_logloss: 0.425468\n",
      "[50]\tvalid_0's binary_logloss: 0.386141\tvalid_1's binary_logloss: 0.424816\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000797 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.441293\tvalid_1's binary_logloss: 0.447553\n",
      "[20]\tvalid_0's binary_logloss: 0.4167\tvalid_1's binary_logloss: 0.430774\n",
      "[30]\tvalid_0's binary_logloss: 0.403908\tvalid_1's binary_logloss: 0.425425\n",
      "[40]\tvalid_0's binary_logloss: 0.39553\tvalid_1's binary_logloss: 0.425029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.424381:  60%|######    | 6/10 [00:01<00:01,  3.17it/s][I 2023-07-17 02:03:56,028] Trial 32 finished with value: 0.4243808317096525 and parameters: {'bagging_fraction': 0.7412866398179849, 'bagging_freq': 4}. Best is trial 27 with value: 0.4243808317096525.\n",
      "bagging, val_score: 0.424381:  60%|######    | 6/10 [00:01<00:01,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's binary_logloss: 0.388442\tvalid_1's binary_logloss: 0.42501\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000782 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.441408\tvalid_1's binary_logloss: 0.446717\n",
      "[20]\tvalid_0's binary_logloss: 0.416505\tvalid_1's binary_logloss: 0.429174\n",
      "[30]\tvalid_0's binary_logloss: 0.403657\tvalid_1's binary_logloss: 0.424997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.424381:  70%|#######   | 7/10 [00:02<00:00,  3.32it/s][I 2023-07-17 02:03:56,299] Trial 33 finished with value: 0.4243808317096525 and parameters: {'bagging_fraction': 0.5864312929872965, 'bagging_freq': 5}. Best is trial 27 with value: 0.4243808317096525.\n",
      "bagging, val_score: 0.424381:  70%|#######   | 7/10 [00:02<00:00,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]\tvalid_0's binary_logloss: 0.395244\tvalid_1's binary_logloss: 0.425068\n",
      "[50]\tvalid_0's binary_logloss: 0.388235\tvalid_1's binary_logloss: 0.425618\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.441507\tvalid_1's binary_logloss: 0.448154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.424381:  80%|########  | 8/10 [00:02<00:00,  3.40it/s][I 2023-07-17 02:03:56,576] Trial 34 finished with value: 0.4243808317096525 and parameters: {'bagging_fraction': 0.8567764609314144, 'bagging_freq': 3}. Best is trial 27 with value: 0.4243808317096525.\n",
      "bagging, val_score: 0.424381:  80%|########  | 8/10 [00:02<00:00,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\tvalid_0's binary_logloss: 0.416747\tvalid_1's binary_logloss: 0.431186\n",
      "[30]\tvalid_0's binary_logloss: 0.403883\tvalid_1's binary_logloss: 0.426846\n",
      "[40]\tvalid_0's binary_logloss: 0.394896\tvalid_1's binary_logloss: 0.425328\n",
      "[50]\tvalid_0's binary_logloss: 0.38716\tvalid_1's binary_logloss: 0.42468\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's binary_logloss: 0.392138\tvalid_1's binary_logloss: 0.424381\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000801 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.424381:  90%|######### | 9/10 [00:02<00:00,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's binary_logloss: 0.441005\tvalid_1's binary_logloss: 0.447434\n",
      "[20]\tvalid_0's binary_logloss: 0.416154\tvalid_1's binary_logloss: 0.430425\n",
      "[30]\tvalid_0's binary_logloss: 0.403372\tvalid_1's binary_logloss: 0.425188\n",
      "[40]\tvalid_0's binary_logloss: 0.394626\tvalid_1's binary_logloss: 0.424684\n",
      "[50]\tvalid_0's binary_logloss: 0.387031\tvalid_1's binary_logloss: 0.423807\n",
      "[60]\tvalid_0's binary_logloss: 0.380946\tvalid_1's binary_logloss: 0.423663\n",
      "Early stopping, best iteration is:\n",
      "[53]\tvalid_0's binary_logloss: 0.219745\tvalid_1's binary_logloss: 0.440925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-17 02:03:56,881] Trial 35 finished with value: 0.4409251551398035 and parameters: {'bagging_fraction': 0.8311818490539997, 'bagging_freq': 6}. Best is trial 27 with value: 0.4243808317096525.\n",
      "bagging, val_score: 0.424381:  90%|######### | 9/10 [00:02<00:00,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000762 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.441337\tvalid_1's binary_logloss: 0.448086\n",
      "[20]\tvalid_0's binary_logloss: 0.41624\tvalid_1's binary_logloss: 0.430215\n",
      "[30]\tvalid_0's binary_logloss: 0.403763\tvalid_1's binary_logloss: 0.426147\n",
      "[40]\tvalid_0's binary_logloss: 0.394863\tvalid_1's binary_logloss: 0.424565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.424381: 100%|##########| 10/10 [00:03<00:00,  3.30it/s][I 2023-07-17 02:03:57,197] Trial 36 finished with value: 0.4409251551398035 and parameters: {'bagging_fraction': 0.557538090996271, 'bagging_freq': 2}. Best is trial 27 with value: 0.4243808317096525.\n",
      "bagging, val_score: 0.424381: 100%|##########| 10/10 [00:03<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's binary_logloss: 0.387282\tvalid_1's binary_logloss: 0.425467\n",
      "[60]\tvalid_0's binary_logloss: 0.380787\tvalid_1's binary_logloss: 0.426247\n",
      "Early stopping, best iteration is:\n",
      "[53]\tvalid_0's binary_logloss: 0.219745\tvalid_1's binary_logloss: 0.440925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.424381:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000800 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.436515\tvalid_1's binary_logloss: 0.443462\n",
      "[20]\tvalid_0's binary_logloss: 0.414314\tvalid_1's binary_logloss: 0.429332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.424381:  17%|#6        | 1/6 [00:00<00:02,  2.44it/s][I 2023-07-17 02:03:57,619] Trial 37 finished with value: 0.4409251551398035 and parameters: {'feature_fraction': 0.8480000000000001}. Best is trial 37 with value: 0.4409251551398035.\n",
      "feature_fraction_stage2, val_score: 0.424381:  17%|#6        | 1/6 [00:00<00:02,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30]\tvalid_0's binary_logloss: 0.402507\tvalid_1's binary_logloss: 0.425488\n",
      "[40]\tvalid_0's binary_logloss: 0.393406\tvalid_1's binary_logloss: 0.424928\n",
      "[50]\tvalid_0's binary_logloss: 0.386349\tvalid_1's binary_logloss: 0.424261\n",
      "[60]\tvalid_0's binary_logloss: 0.379573\tvalid_1's binary_logloss: 0.424412\n",
      "Early stopping, best iteration is:\n",
      "[53]\tvalid_0's binary_logloss: 0.219745\tvalid_1's binary_logloss: 0.440925\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000787 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.436515\tvalid_1's binary_logloss: 0.443462\n",
      "[20]\tvalid_0's binary_logloss: 0.414314\tvalid_1's binary_logloss: 0.429332\n",
      "[30]\tvalid_0's binary_logloss: 0.402507\tvalid_1's binary_logloss: 0.425488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.424381:  33%|###3      | 2/6 [00:00<00:01,  2.38it/s][I 2023-07-17 02:03:58,044] Trial 38 finished with value: 0.4409251551398035 and parameters: {'feature_fraction': 0.88}. Best is trial 37 with value: 0.4409251551398035.\n",
      "feature_fraction_stage2, val_score: 0.424381:  33%|###3      | 2/6 [00:00<00:01,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]\tvalid_0's binary_logloss: 0.393406\tvalid_1's binary_logloss: 0.424928\n",
      "[50]\tvalid_0's binary_logloss: 0.386349\tvalid_1's binary_logloss: 0.424261\n",
      "[60]\tvalid_0's binary_logloss: 0.379573\tvalid_1's binary_logloss: 0.424412\n",
      "Early stopping, best iteration is:\n",
      "[53]\tvalid_0's binary_logloss: 0.219745\tvalid_1's binary_logloss: 0.440925\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.443525\tvalid_1's binary_logloss: 0.449815\n",
      "[20]\tvalid_0's binary_logloss: 0.416834\tvalid_1's binary_logloss: 0.430322\n",
      "[30]\tvalid_0's binary_logloss: 0.40403\tvalid_1's binary_logloss: 0.425741\n",
      "[40]\tvalid_0's binary_logloss: 0.394967\tvalid_1's binary_logloss: 0.425308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.424381:  50%|#####     | 3/6 [00:01<00:01,  2.77it/s][I 2023-07-17 02:03:58,334] Trial 39 finished with value: 0.4409251551398035 and parameters: {'feature_fraction': 0.7200000000000001}. Best is trial 37 with value: 0.4409251551398035.\n",
      "feature_fraction_stage2, val_score: 0.424381:  50%|#####     | 3/6 [00:01<00:01,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's binary_logloss: 0.387457\tvalid_1's binary_logloss: 0.425095\n",
      "[60]\tvalid_0's binary_logloss: 0.381268\tvalid_1's binary_logloss: 0.425668\n",
      "Early stopping, best iteration is:\n",
      "[53]\tvalid_0's binary_logloss: 0.219745\tvalid_1's binary_logloss: 0.440925\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001399 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.437782\tvalid_1's binary_logloss: 0.444642\n",
      "[20]\tvalid_0's binary_logloss: 0.414677\tvalid_1's binary_logloss: 0.429804\n",
      "[30]\tvalid_0's binary_logloss: 0.402317\tvalid_1's binary_logloss: 0.426128\n",
      "[40]\tvalid_0's binary_logloss: 0.393714\tvalid_1's binary_logloss: 0.425436\n",
      "[50]\tvalid_0's binary_logloss: 0.386112\tvalid_1's binary_logloss: 0.425653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.424381:  67%|######6   | 4/6 [00:01<00:00,  3.04it/s][I 2023-07-17 02:03:58,614] Trial 40 finished with value: 0.4409251551398035 and parameters: {'feature_fraction': 0.8160000000000001}. Best is trial 37 with value: 0.4409251551398035.\n",
      "feature_fraction_stage2, val_score: 0.424381:  67%|######6   | 4/6 [00:01<00:00,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60]\tvalid_0's binary_logloss: 0.379628\tvalid_1's binary_logloss: 0.425464\n",
      "Early stopping, best iteration is:\n",
      "[53]\tvalid_0's binary_logloss: 0.219745\tvalid_1's binary_logloss: 0.440925\n",
      "[LightGBM] [Info] Number of positive: 5076, number of negative: 17874\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000769 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 22950, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221176 -> initscore=-1.258824\n",
      "[LightGBM] [Info] Start training from score -1.258824\n",
      "[10]\tvalid_0's binary_logloss: 0.441231\tvalid_1's binary_logloss: 0.447832\n",
      "[20]\tvalid_0's binary_logloss: 0.416449\tvalid_1's binary_logloss: 0.4302\n",
      "[30]\tvalid_0's binary_logloss: 0.403423\tvalid_1's binary_logloss: 0.426256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-07-17 02:03:58,853] Trial 41 failed with parameters: {'feature_fraction': 0.784} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/ml_utils/venv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/workspaces/ml_utils/venv/lib/python3.10/site-packages/optuna/integration/_lightgbm_tuner/optimize.py\", line 240, in __call__\n",
      "    booster = lgb.train(self.lgbm_params, train_set, **kwargs)\n",
      "  File \"/workspaces/ml_utils/venv/lib/python3.10/site-packages/lightgbm/engine.py\", line 292, in train\n",
      "    booster.update(fobj=fobj)\n",
      "  File \"/workspaces/ml_utils/venv/lib/python3.10/site-packages/lightgbm/basic.py\", line 3021, in update\n",
      "    _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2023-07-17 02:03:58,854] Trial 41 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]\tvalid_0's binary_logloss: 0.394397\tvalid_1's binary_logloss: 0.424498\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m lgb_optuna\u001b[39m.\u001b[39;49mtrain(params, dtrain, valid_sets\u001b[39m=\u001b[39;49m[dtrain, dval], \n\u001b[1;32m      2\u001b[0m                          callbacks\u001b[39m=\u001b[39;49m[lgb\u001b[39m.\u001b[39;49mearly_stopping(\u001b[39m10\u001b[39;49m),lgb\u001b[39m.\u001b[39;49mlog_evaluation(period\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)])\n",
      "File \u001b[0;32m/workspaces/ml_utils/venv/lib/python3.10/site-packages/optuna/integration/_lightgbm_tuner/__init__.py:37\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m _imports\u001b[39m.\u001b[39mcheck()\n\u001b[1;32m     36\u001b[0m auto_booster \u001b[39m=\u001b[39m LightGBMTuner(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 37\u001b[0m auto_booster\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m     38\u001b[0m \u001b[39mreturn\u001b[39;00m auto_booster\u001b[39m.\u001b[39mget_best_booster()\n",
      "File \u001b[0;32m/workspaces/ml_utils/venv/lib/python3.10/site-packages/optuna/integration/_lightgbm_tuner/optimize.py:539\u001b[0m, in \u001b[0;36m_LightGBMBaseTuner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtune_num_leaves()\n\u001b[1;32m    538\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtune_bagging()\n\u001b[0;32m--> 539\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtune_feature_fraction_stage2()\n\u001b[1;32m    540\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtune_regularization_factors()\n\u001b[1;32m    541\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtune_min_data_in_leaf()\n",
      "File \u001b[0;32m/workspaces/ml_utils/venv/lib/python3.10/site-packages/optuna/integration/_lightgbm_tuner/optimize.py:588\u001b[0m, in \u001b[0;36m_LightGBMBaseTuner.tune_feature_fraction_stage2\u001b[0;34m(self, n_trials)\u001b[0m\n\u001b[1;32m    585\u001b[0m param_values \u001b[39m=\u001b[39m [val \u001b[39mfor\u001b[39;00m val \u001b[39min\u001b[39;00m param_values \u001b[39mif\u001b[39;00m val \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.4\u001b[39m \u001b[39mand\u001b[39;00m val \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m]\n\u001b[1;32m    587\u001b[0m sampler \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39msamplers\u001b[39m.\u001b[39mGridSampler({param_name: param_values})\n\u001b[0;32m--> 588\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tune_params([param_name], \u001b[39mlen\u001b[39;49m(param_values), sampler, \u001b[39m\"\u001b[39;49m\u001b[39mfeature_fraction_stage2\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/workspaces/ml_utils/venv/lib/python3.10/site-packages/optuna/integration/_lightgbm_tuner/optimize.py:644\u001b[0m, in \u001b[0;36m_LightGBMBaseTuner._tune_params\u001b[0;34m(self, target_param_names, n_trials, sampler, step_name)\u001b[0m\n\u001b[1;32m    642\u001b[0m     _timeout \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[39mif\u001b[39;00m _n_trials \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 644\u001b[0m     study\u001b[39m.\u001b[39;49moptimize(\n\u001b[1;32m    645\u001b[0m         objective,\n\u001b[1;32m    646\u001b[0m         n_trials\u001b[39m=\u001b[39;49m_n_trials,\n\u001b[1;32m    647\u001b[0m         timeout\u001b[39m=\u001b[39;49m_timeout,\n\u001b[1;32m    648\u001b[0m         catch\u001b[39m=\u001b[39;49m(),\n\u001b[1;32m    649\u001b[0m         callbacks\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optuna_callbacks,\n\u001b[1;32m    650\u001b[0m     )\n\u001b[1;32m    652\u001b[0m \u001b[39mif\u001b[39;00m pbar:\n\u001b[1;32m    653\u001b[0m     pbar\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/workspaces/ml_utils/venv/lib/python3.10/site-packages/optuna/study/study.py:443\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m     _optimize(\n\u001b[1;32m    444\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    445\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    446\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    447\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    448\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    449\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    450\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    451\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    452\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    453\u001b[0m     )\n",
      "File \u001b[0;32m/workspaces/ml_utils/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/workspaces/ml_utils/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/workspaces/ml_utils/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/workspaces/ml_utils/venv/lib/python3.10/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "File \u001b[0;32m/workspaces/ml_utils/venv/lib/python3.10/site-packages/optuna/integration/_lightgbm_tuner/optimize.py:240\u001b[0m, in \u001b[0;36m_OptunaObjective.__call__\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    238\u001b[0m kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlgbm_kwargs)\n\u001b[1;32m    239\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mvalid_sets\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copy_valid_sets(kwargs[\u001b[39m\"\u001b[39m\u001b[39mvalid_sets\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> 240\u001b[0m booster \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39;49mtrain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlgbm_params, train_set, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    242\u001b[0m val_score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_booster_best_score(booster)\n\u001b[1;32m    243\u001b[0m elapsed_secs \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m/workspaces/ml_utils/venv/lib/python3.10/site-packages/lightgbm/engine.py:292\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    285\u001b[0m     cb(callback\u001b[39m.\u001b[39mCallbackEnv(model\u001b[39m=\u001b[39mbooster,\n\u001b[1;32m    286\u001b[0m                             params\u001b[39m=\u001b[39mparams,\n\u001b[1;32m    287\u001b[0m                             iteration\u001b[39m=\u001b[39mi,\n\u001b[1;32m    288\u001b[0m                             begin_iteration\u001b[39m=\u001b[39minit_iteration,\n\u001b[1;32m    289\u001b[0m                             end_iteration\u001b[39m=\u001b[39minit_iteration \u001b[39m+\u001b[39m num_boost_round,\n\u001b[1;32m    290\u001b[0m                             evaluation_result_list\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m))\n\u001b[0;32m--> 292\u001b[0m booster\u001b[39m.\u001b[39;49mupdate(fobj\u001b[39m=\u001b[39;49mfobj)\n\u001b[1;32m    294\u001b[0m evaluation_result_list \u001b[39m=\u001b[39m []\n\u001b[1;32m    295\u001b[0m \u001b[39m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/ml_utils/venv/lib/python3.10/site-packages/lightgbm/basic.py:3021\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3019\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   3020\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(\u001b[39m'\u001b[39m\u001b[39mCannot update due to null objective function.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 3021\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterUpdateOneIter(\n\u001b[1;32m   3022\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   3023\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(is_finished)))\n\u001b[1;32m   3024\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__is_predicted_cur_iter \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__num_dataset)]\n\u001b[1;32m   3025\u001b[0m \u001b[39mreturn\u001b[39;00m is_finished\u001b[39m.\u001b[39mvalue \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = lgb_optuna.train(params, dtrain, valid_sets=[dtrain, dval], \n",
    "                         callbacks=[lgb.early_stopping(3),lgb.log_evaluation(period=10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBMTunerCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-17 02:00:11,015] A new study created in memory with name: no-name-260cf369-13cb-49b5-95c2-abaa637b191e\n",
      "feature_fraction, val_score: 0.452732:  14%|#4        | 1/7 [00:00<00:01,  4.42it/s][I 2023-07-17 02:00:11,248] Trial 0 finished with value: 0.4527324975027975 and parameters: {'feature_fraction': 0.6}. Best is trial 0 with value: 0.4527324975027975.\n",
      "feature_fraction, val_score: 0.452732:  14%|#4        | 1/7 [00:00<00:01,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000487 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000475 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000473 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000488 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000477 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.452732 + 0.00503915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.443360:  29%|##8       | 2/7 [00:00<00:01,  4.45it/s][I 2023-07-17 02:00:11,472] Trial 1 finished with value: 0.44336048231327946 and parameters: {'feature_fraction': 1.0}. Best is trial 1 with value: 0.44336048231327946.\n",
      "feature_fraction, val_score: 0.443360:  29%|##8       | 2/7 [00:00<00:01,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44336 + 0.00484318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.443360:  43%|####2     | 3/7 [00:00<00:00,  4.75it/s][I 2023-07-17 02:00:11,666] Trial 2 finished with value: 0.461475286031315 and parameters: {'feature_fraction': 0.4}. Best is trial 1 with value: 0.44336048231327946.\n",
      "feature_fraction, val_score: 0.443360:  43%|####2     | 3/7 [00:00<00:00,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000340 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000314 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000316 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44336 + 0.00484318\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000635 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.443360:  57%|#####7    | 4/7 [00:00<00:00,  4.67it/s][I 2023-07-17 02:00:11,885] Trial 3 finished with value: 0.44977883762880405 and parameters: {'feature_fraction': 0.8}. Best is trial 1 with value: 0.44336048231327946.\n",
      "feature_fraction, val_score: 0.443360:  57%|#####7    | 4/7 [00:00<00:00,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000635 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000648 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44336 + 0.00484318\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000948 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000419 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000386 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000407 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.443360:  71%|#######1  | 5/7 [00:01<00:00,  3.67it/s][I 2023-07-17 02:00:12,261] Trial 4 finished with value: 0.45311140373266034 and parameters: {'feature_fraction': 0.5}. Best is trial 1 with value: 0.44336048231327946.\n",
      "feature_fraction, val_score: 0.443360:  71%|#######1  | 5/7 [00:01<00:00,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44336 + 0.00484318\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002496 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000613 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002450 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002321 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.443360:  86%|########5 | 6/7 [00:01<00:00,  3.73it/s][I 2023-07-17 02:00:12,522] Trial 5 finished with value: 0.45231432447441416 and parameters: {'feature_fraction': 0.7}. Best is trial 1 with value: 0.44336048231327946.\n",
      "feature_fraction, val_score: 0.443360:  86%|########5 | 6/7 [00:01<00:00,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44336 + 0.00484318\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000719 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000689 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000690 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.443360: 100%|##########| 7/7 [00:01<00:00,  3.89it/s][I 2023-07-17 02:00:12,755] Trial 6 finished with value: 0.4443635332107501 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 1 with value: 0.44336048231327946.\n",
      "feature_fraction, val_score: 0.443360: 100%|##########| 7/7 [00:01<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44336 + 0.00484318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.443360:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000729 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.443360:   5%|5         | 1/20 [00:00<00:13,  1.42it/s][I 2023-07-17 02:00:13,465] Trial 7 finished with value: 0.4436574745006667 and parameters: {'num_leaves': 224}. Best is trial 7 with value: 0.4436574745006667.\n",
      "num_leaves, val_score: 0.443360:   5%|5         | 1/20 [00:00<00:13,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44336 + 0.00484318\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000741 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.443360:  10%|#         | 2/20 [00:01<00:11,  1.51it/s][I 2023-07-17 02:00:14,096] Trial 8 finished with value: 0.4437865250036344 and parameters: {'num_leaves': 248}. Best is trial 7 with value: 0.4436574745006667.\n",
      "num_leaves, val_score: 0.443360:  10%|#         | 2/20 [00:01<00:11,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44336 + 0.00484318\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000773 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000703 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.443360:  15%|#5        | 3/20 [00:02<00:11,  1.43it/s][I 2023-07-17 02:00:14,844] Trial 9 finished with value: 0.4436639024681246 and parameters: {'num_leaves': 223}. Best is trial 7 with value: 0.4436574745006667.\n",
      "num_leaves, val_score: 0.443360:  20%|##        | 4/20 [00:02<00:07,  2.05it/s][I 2023-07-17 02:00:15,006] Trial 10 finished with value: 0.44699433487105383 and parameters: {'num_leaves': 9}. Best is trial 7 with value: 0.4436574745006667.\n",
      "num_leaves, val_score: 0.443360:  20%|##        | 4/20 [00:02<00:07,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44336 + 0.00484318\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000702 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44336 + 0.00484318\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.442420:  25%|##5       | 5/20 [00:02<00:07,  2.07it/s][I 2023-07-17 02:00:15,480] Trial 11 finished with value: 0.4424199661495737 and parameters: {'num_leaves': 144}. Best is trial 11 with value: 0.4424199661495737.\n",
      "num_leaves, val_score: 0.442420:  25%|##5       | 5/20 [00:02<00:07,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44242 + 0.00550607\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000703 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.442420:  30%|###       | 6/20 [00:03<00:06,  2.19it/s][I 2023-07-17 02:00:15,884] Trial 12 finished with value: 0.4424636372603194 and parameters: {'num_leaves': 111}. Best is trial 11 with value: 0.4424199661495737.\n",
      "num_leaves, val_score: 0.442420:  30%|###       | 6/20 [00:03<00:06,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44242 + 0.00550607\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000702 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.442420:  35%|###5      | 7/20 [00:03<00:05,  2.26it/s][I 2023-07-17 02:00:16,300] Trial 13 finished with value: 0.44244693322188205 and parameters: {'num_leaves': 117}. Best is trial 11 with value: 0.4424199661495737.\n",
      "num_leaves, val_score: 0.442420:  35%|###5      | 7/20 [00:03<00:05,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44242 + 0.00550607\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000719 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000811 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.442420:  40%|####      | 8/20 [00:04<00:05,  2.15it/s][I 2023-07-17 02:00:16,815] Trial 14 finished with value: 0.4427756316952605 and parameters: {'num_leaves': 128}. Best is trial 11 with value: 0.4424199661495737.\n",
      "num_leaves, val_score: 0.442420:  40%|####      | 8/20 [00:04<00:05,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44242 + 0.00550607\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000697 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000703 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.442420:  45%|####5     | 9/20 [00:04<00:04,  2.37it/s][I 2023-07-17 02:00:17,142] Trial 15 finished with value: 0.44259271455147975 and parameters: {'num_leaves': 76}. Best is trial 11 with value: 0.4424199661495737.\n",
      "num_leaves, val_score: 0.442420:  45%|####5     | 9/20 [00:04<00:04,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44242 + 0.00550607\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000717 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.442420:  50%|#####     | 10/20 [00:04<00:04,  2.20it/s][I 2023-07-17 02:00:17,669] Trial 16 finished with value: 0.4429103421809543 and parameters: {'num_leaves': 174}. Best is trial 11 with value: 0.4424199661495737.\n",
      "num_leaves, val_score: 0.442420:  50%|#####     | 10/20 [00:04<00:04,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44242 + 0.00550607\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000719 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.442420:  55%|#####5    | 11/20 [00:05<00:04,  2.12it/s][I 2023-07-17 02:00:18,180] Trial 17 finished with value: 0.4425754833208092 and parameters: {'num_leaves': 170}. Best is trial 11 with value: 0.4424199661495737.\n",
      "num_leaves, val_score: 0.442420:  55%|#####5    | 11/20 [00:05<00:04,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44242 + 0.00550607\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.442420:  60%|######    | 12/20 [00:05<00:03,  2.27it/s][I 2023-07-17 02:00:18,545] Trial 18 finished with value: 0.4426956181139131 and parameters: {'num_leaves': 75}. Best is trial 11 with value: 0.4424199661495737.\n",
      "num_leaves, val_score: 0.442420:  60%|######    | 12/20 [00:05<00:03,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44242 + 0.00550607\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000880 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000723 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.442070:  65%|######5   | 13/20 [00:06<00:03,  2.07it/s][I 2023-07-17 02:00:19,131] Trial 19 finished with value: 0.4420701223696448 and parameters: {'num_leaves': 164}. Best is trial 19 with value: 0.4420701223696448.\n",
      "num_leaves, val_score: 0.442070:  65%|######5   | 13/20 [00:06<00:03,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569106\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000697 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000716 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.442070:  70%|#######   | 14/20 [00:06<00:03,  1.94it/s][I 2023-07-17 02:00:19,719] Trial 20 finished with value: 0.4429661547722607 and parameters: {'num_leaves': 177}. Best is trial 19 with value: 0.4420701223696448.\n",
      "num_leaves, val_score: 0.442070:  70%|#######   | 14/20 [00:06<00:03,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569106\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000897 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.442070:  75%|#######5  | 15/20 [00:07<00:02,  1.93it/s][I 2023-07-17 02:00:20,241] Trial 21 finished with value: 0.4425174045703786 and parameters: {'num_leaves': 153}. Best is trial 19 with value: 0.4420701223696448.\n",
      "num_leaves, val_score: 0.442070:  75%|#######5  | 15/20 [00:07<00:02,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569106\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.442070:  80%|########  | 16/20 [00:07<00:01,  2.13it/s][I 2023-07-17 02:00:20,597] Trial 22 finished with value: 0.4424259843973708 and parameters: {'num_leaves': 89}. Best is trial 19 with value: 0.4420701223696448.\n",
      "num_leaves, val_score: 0.442070:  80%|########  | 16/20 [00:07<00:01,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569106\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000728 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000740 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.442070:  85%|########5 | 17/20 [00:08<00:01,  2.21it/s][I 2023-07-17 02:00:21,013] Trial 23 finished with value: 0.44283193625286044 and parameters: {'num_leaves': 73}. Best is trial 19 with value: 0.4420701223696448.\n",
      "num_leaves, val_score: 0.442070:  85%|########5 | 17/20 [00:08<00:01,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569106\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005465 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.442070:  90%|######### | 18/20 [00:08<00:00,  2.14it/s][I 2023-07-17 02:00:21,516] Trial 24 finished with value: 0.4423634271839241 and parameters: {'num_leaves': 147}. Best is trial 19 with value: 0.4420701223696448.\n",
      "num_leaves, val_score: 0.442070:  90%|######### | 18/20 [00:08<00:00,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569106\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000705 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000703 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000751 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.442070:  95%|#########5| 19/20 [00:09<00:00,  2.09it/s][I 2023-07-17 02:00:22,019] Trial 25 finished with value: 0.4424940471855945 and parameters: {'num_leaves': 151}. Best is trial 19 with value: 0.4420701223696448.\n",
      "num_leaves, val_score: 0.442070:  95%|#########5| 19/20 [00:09<00:00,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569106\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.442070: 100%|##########| 20/20 [00:10<00:00,  1.58it/s][I 2023-07-17 02:00:23,011] Trial 26 finished with value: 0.44280038692046003 and parameters: {'num_leaves': 186}. Best is trial 19 with value: 0.4420701223696448.\n",
      "num_leaves, val_score: 0.442070: 100%|##########| 20/20 [00:10<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.442070:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000716 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.442070:  10%|#         | 1/10 [00:00<00:05,  1.51it/s][I 2023-07-17 02:00:23,682] Trial 27 finished with value: 0.444326828059699 and parameters: {'bagging_fraction': 0.44546904318206293, 'bagging_freq': 3}. Best is trial 27 with value: 0.444326828059699.\n",
      "bagging, val_score: 0.442070:  10%|#         | 1/10 [00:00<00:05,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569106\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002341 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.442070:  20%|##        | 2/10 [00:01<00:05,  1.50it/s][I 2023-07-17 02:00:24,349] Trial 28 finished with value: 0.44318158809504427 and parameters: {'bagging_fraction': 0.9257335789622447, 'bagging_freq': 7}. Best is trial 28 with value: 0.44318158809504427.\n",
      "bagging, val_score: 0.442070:  20%|##        | 2/10 [00:01<00:05,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569106\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003491 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.442070:  30%|###       | 3/10 [00:02<00:05,  1.37it/s][I 2023-07-17 02:00:25,152] Trial 29 finished with value: 0.44315365720914474 and parameters: {'bagging_fraction': 0.9741955081760625, 'bagging_freq': 1}. Best is trial 29 with value: 0.44315365720914474.\n",
      "bagging, val_score: 0.442070:  30%|###       | 3/10 [00:02<00:05,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569106\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000875 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005068 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.442070:  40%|####      | 4/10 [00:02<00:03,  1.55it/s][I 2023-07-17 02:00:25,667] Trial 30 finished with value: 0.44456640783276863 and parameters: {'bagging_fraction': 0.6155420004827398, 'bagging_freq': 7}. Best is trial 29 with value: 0.44315365720914474.\n",
      "bagging, val_score: 0.442070:  40%|####      | 4/10 [00:02<00:03,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569106\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.442070:  50%|#####     | 5/10 [00:03<00:03,  1.65it/s][I 2023-07-17 02:00:26,203] Trial 31 finished with value: 0.44414425048996353 and parameters: {'bagging_fraction': 0.7492253157955532, 'bagging_freq': 4}. Best is trial 29 with value: 0.44315365720914474.\n",
      "bagging, val_score: 0.442070:  50%|#####     | 5/10 [00:03<00:03,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569106\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000689 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.442070:  60%|######    | 6/10 [00:03<00:02,  1.77it/s][I 2023-07-17 02:00:26,694] Trial 32 finished with value: 0.4430925968528328 and parameters: {'bagging_fraction': 0.4370542117249007, 'bagging_freq': 1}. Best is trial 32 with value: 0.4430925968528328.\n",
      "bagging, val_score: 0.442070:  60%|######    | 6/10 [00:03<00:02,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569106\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000689 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000702 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000745 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004339 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.442070:  70%|#######   | 7/10 [00:04<00:01,  1.70it/s][I 2023-07-17 02:00:27,325] Trial 33 finished with value: 0.4441451605322574 and parameters: {'bagging_fraction': 0.7932529354050741, 'bagging_freq': 5}. Best is trial 32 with value: 0.4430925968528328.\n",
      "bagging, val_score: 0.442070:  70%|#######   | 7/10 [00:04<00:01,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569106\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000716 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000694 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.442070:  80%|########  | 8/10 [00:05<00:01,  1.55it/s][I 2023-07-17 02:00:28,095] Trial 34 finished with value: 0.4450546526569979 and parameters: {'bagging_fraction': 0.6066000617170673, 'bagging_freq': 5}. Best is trial 32 with value: 0.4430925968528328.\n",
      "bagging, val_score: 0.442070:  80%|########  | 8/10 [00:05<00:01,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569106\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000717 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000687 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000876 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000687 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.442070:  90%|######### | 9/10 [00:05<00:00,  1.56it/s][I 2023-07-17 02:00:28,725] Trial 35 finished with value: 0.44426787509369364 and parameters: {'bagging_fraction': 0.8523872952774495, 'bagging_freq': 3}. Best is trial 32 with value: 0.4430925968528328.\n",
      "bagging, val_score: 0.442070:  90%|######### | 9/10 [00:05<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569106\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000717 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000768 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.442070: 100%|##########| 10/10 [00:06<00:00,  1.63it/s][I 2023-07-17 02:00:29,277] Trial 36 finished with value: 0.4427999403573074 and parameters: {'bagging_fraction': 0.9982160183801443, 'bagging_freq': 2}. Best is trial 36 with value: 0.4427999403573074.\n",
      "bagging, val_score: 0.442070: 100%|##########| 10/10 [00:06<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.442070:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000700 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000698 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.442070:  33%|###3      | 1/3 [00:00<00:01,  1.82it/s][I 2023-07-17 02:00:29,830] Trial 37 finished with value: 0.4444444591191538 and parameters: {'feature_fraction': 0.92}. Best is trial 37 with value: 0.4444444591191538.\n",
      "feature_fraction_stage2, val_score: 0.442070:  33%|###3      | 1/3 [00:00<00:01,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569106\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000714 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.442070:  67%|######6   | 2/3 [00:01<00:00,  1.88it/s][I 2023-07-17 02:00:30,349] Trial 38 finished with value: 0.4420701223696447 and parameters: {'feature_fraction': 0.9840000000000001}. Best is trial 38 with value: 0.4420701223696447.\n",
      "feature_fraction_stage2, val_score: 0.442070:  67%|######6   | 2/3 [00:01<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569106\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003198 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000737 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000714 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.442070: 100%|##########| 3/3 [00:01<00:00,  1.88it/s][I 2023-07-17 02:00:30,883] Trial 39 finished with value: 0.4429628092673184 and parameters: {'feature_fraction': 0.9520000000000001}. Best is trial 38 with value: 0.4420701223696447.\n",
      "feature_fraction_stage2, val_score: 0.442070: 100%|##########| 3/3 [00:01<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.442070:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000688 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000694 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.442070:   5%|5         | 1/20 [00:00<00:11,  1.58it/s][I 2023-07-17 02:00:31,521] Trial 40 finished with value: 0.4420701135579133 and parameters: {'lambda_l1': 4.808788182919305e-08, 'lambda_l2': 6.573197667225549e-06}. Best is trial 40 with value: 0.4420701135579133.\n",
      "regularization_factors, val_score: 0.442070:   5%|5         | 1/20 [00:00<00:11,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569107\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000716 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.442070:  10%|#         | 2/20 [00:01<00:10,  1.75it/s][I 2023-07-17 02:00:32,053] Trial 41 finished with value: 0.4420695596754414 and parameters: {'lambda_l1': 1.503447827323367e-08, 'lambda_l2': 4.2982984219247195e-06}. Best is trial 41 with value: 0.4420695596754414.\n",
      "regularization_factors, val_score: 0.442070:  10%|#         | 2/20 [00:01<00:10,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569195\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000750 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.442070:  15%|#5        | 3/20 [00:01<00:10,  1.68it/s][I 2023-07-17 02:00:32,674] Trial 42 finished with value: 0.4420701133148567 and parameters: {'lambda_l1': 2.85626400839504e-08, 'lambda_l2': 4.406915912207612e-06}. Best is trial 41 with value: 0.4420695596754414.\n",
      "regularization_factors, val_score: 0.442070:  15%|#5        | 3/20 [00:01<00:10,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569195\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.442070:  20%|##        | 4/20 [00:02<00:10,  1.47it/s][I 2023-07-17 02:00:33,480] Trial 43 finished with value: 0.4420695596570404 and parameters: {'lambda_l1': 1.2572457004304995e-08, 'lambda_l2': 4.141049064368032e-06}. Best is trial 43 with value: 0.4420695596570404.\n",
      "regularization_factors, val_score: 0.442070:  20%|##        | 4/20 [00:02<00:10,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569195\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021443 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000690 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000712 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000717 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.442070:  25%|##5       | 5/20 [00:03<00:10,  1.48it/s][I 2023-07-17 02:00:34,154] Trial 44 finished with value: 0.44207011322630674 and parameters: {'lambda_l1': 1.0900425230980108e-08, 'lambda_l2': 3.666147195987956e-06}. Best is trial 43 with value: 0.4420695596570404.\n",
      "regularization_factors, val_score: 0.442070:  25%|##5       | 5/20 [00:03<00:10,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569195\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000694 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000697 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.442070:  30%|###       | 6/20 [00:03<00:08,  1.60it/s][I 2023-07-17 02:00:34,676] Trial 45 finished with value: 0.4420701132723055 and parameters: {'lambda_l1': 1.3314498400630136e-08, 'lambda_l2': 4.073538536207759e-06}. Best is trial 43 with value: 0.4420695596570404.\n",
      "regularization_factors, val_score: 0.442070:  30%|###       | 6/20 [00:03<00:08,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569195\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000694 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.442070:  35%|###5      | 7/20 [00:04<00:07,  1.68it/s][I 2023-07-17 02:00:35,211] Trial 46 finished with value: 0.4420701131454722 and parameters: {'lambda_l1': 1.1410940504428098e-08, 'lambda_l2': 2.935043861671223e-06}. Best is trial 43 with value: 0.4420695596570404.\n",
      "regularization_factors, val_score: 0.442070:  35%|###5      | 7/20 [00:04<00:07,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569195\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000702 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000688 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.442070:  40%|####      | 8/20 [00:05<00:07,  1.60it/s][I 2023-07-17 02:00:35,895] Trial 47 finished with value: 0.44207011329683493 and parameters: {'lambda_l1': 1.0313420567131669e-08, 'lambda_l2': 4.271776759401452e-06}. Best is trial 43 with value: 0.4420695596570404.\n",
      "regularization_factors, val_score: 0.442070:  40%|####      | 8/20 [00:05<00:07,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569195\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.442070:  45%|####5     | 9/20 [00:05<00:06,  1.68it/s][I 2023-07-17 02:00:36,424] Trial 48 finished with value: 0.44207011316191547 and parameters: {'lambda_l1': 1.2014804852915114e-08, 'lambda_l2': 3.0773786253971515e-06}. Best is trial 43 with value: 0.4420695596570404.\n",
      "regularization_factors, val_score: 0.442070:  45%|####5     | 9/20 [00:05<00:06,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569195\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000689 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000702 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000702 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.442070:  50%|#####     | 10/20 [00:06<00:05,  1.74it/s][I 2023-07-17 02:00:36,957] Trial 49 finished with value: 0.44207011304384736 and parameters: {'lambda_l1': 1.61189003196224e-08, 'lambda_l2': 2.090775602745467e-06}. Best is trial 43 with value: 0.4420695596570404.\n",
      "regularization_factors, val_score: 0.442070:  50%|#####     | 10/20 [00:06<00:05,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569195\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000693 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002157 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.442070:  55%|#####5    | 11/20 [00:06<00:05,  1.78it/s][I 2023-07-17 02:00:37,486] Trial 50 finished with value: 0.4420701223937473 and parameters: {'lambda_l1': 1.3090333548756296e-08, 'lambda_l2': 1.2391901758384845e-07}. Best is trial 43 with value: 0.4420695596570404.\n",
      "regularization_factors, val_score: 0.442070:  55%|#####5    | 11/20 [00:06<00:05,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569195\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000714 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000705 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.442070:  60%|######    | 12/20 [00:07<00:04,  1.72it/s][I 2023-07-17 02:00:38,114] Trial 51 finished with value: 0.44206955968605915 and parameters: {'lambda_l1': 1.063636567339854e-08, 'lambda_l2': 4.385034051254178e-06}. Best is trial 43 with value: 0.4420695596570404.\n",
      "regularization_factors, val_score: 0.442070:  60%|######    | 12/20 [00:07<00:04,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569195\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000716 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.442070:  65%|######5   | 13/20 [00:07<00:04,  1.70it/s][I 2023-07-17 02:00:38,718] Trial 52 finished with value: 0.4420701134667203 and parameters: {'lambda_l1': 1.2966885651910055e-08, 'lambda_l2': 5.886243768375581e-06}. Best is trial 43 with value: 0.4420695596570404.\n",
      "regularization_factors, val_score: 0.442070:  65%|######5   | 13/20 [00:07<00:04,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569195\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000690 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000698 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002793 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.442070:  70%|#######   | 14/20 [00:08<00:03,  1.75it/s][I 2023-07-17 02:00:39,248] Trial 53 finished with value: 0.44237262415820416 and parameters: {'lambda_l1': 7.07560581442117e-07, 'lambda_l2': 0.0009365027996837425}. Best is trial 43 with value: 0.4420695596570404.\n",
      "regularization_factors, val_score: 0.442070:  70%|#######   | 14/20 [00:08<00:03,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569195\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000714 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000692 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000700 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.442070:  75%|#######5  | 15/20 [00:08<00:02,  1.77it/s][I 2023-07-17 02:00:39,802] Trial 54 finished with value: 0.4427190891219931 and parameters: {'lambda_l1': 0.004951741693365655, 'lambda_l2': 0.9621314427032533}. Best is trial 43 with value: 0.4420695596570404.\n",
      "regularization_factors, val_score: 0.442070:  75%|#######5  | 15/20 [00:08<00:02,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569195\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000735 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.442070:  80%|########  | 16/20 [00:09<00:02,  1.68it/s][I 2023-07-17 02:00:40,461] Trial 55 finished with value: 0.4420695593513052 and parameters: {'lambda_l1': 1.4299183901170552e-06, 'lambda_l2': 3.3642476428392784e-07}. Best is trial 55 with value: 0.4420695593513052.\n",
      "regularization_factors, val_score: 0.442070:  80%|########  | 16/20 [00:09<00:02,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569195\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000728 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000716 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.442070:  85%|########5 | 17/20 [00:10<00:01,  1.69it/s][I 2023-07-17 02:00:41,049] Trial 56 finished with value: 0.442070112975739 and parameters: {'lambda_l1': 1.3882571608501857e-06, 'lambda_l2': 2.7843864669534243e-07}. Best is trial 55 with value: 0.4420695593513052.\n",
      "regularization_factors, val_score: 0.442070:  85%|########5 | 17/20 [00:10<00:01,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569195\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000716 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.442070:  90%|######### | 18/20 [00:10<00:01,  1.71it/s][I 2023-07-17 02:00:41,618] Trial 57 finished with value: 0.4420701130805432 and parameters: {'lambda_l1': 2.526837002108009e-06, 'lambda_l2': 8.386679721191491e-08}. Best is trial 55 with value: 0.4420695593513052.\n",
      "regularization_factors, val_score: 0.442070:  90%|######### | 18/20 [00:10<00:01,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569195\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000715 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000812 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002904 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000720 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.442070:  95%|#########5| 19/20 [00:11<00:00,  1.74it/s][I 2023-07-17 02:00:42,173] Trial 58 finished with value: 0.44207011291748904 and parameters: {'lambda_l1': 7.447479241970494e-07, 'lambda_l2': 1.988391284705041e-07}. Best is trial 55 with value: 0.4420695593513052.\n",
      "regularization_factors, val_score: 0.442070:  95%|#########5| 19/20 [00:11<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569195\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000743 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000754 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000745 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000714 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.442070: 100%|##########| 20/20 [00:11<00:00,  1.75it/s][I 2023-07-17 02:00:42,732] Trial 59 finished with value: 0.44207011291746034 and parameters: {'lambda_l1': 7.883908202432746e-07, 'lambda_l2': 1.237592371128154e-07}. Best is trial 55 with value: 0.4420695593513052.\n",
      "regularization_factors, val_score: 0.442070: 100%|##########| 20/20 [00:11<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.44207 + 0.00569195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.442070:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.441598:  20%|##        | 1/5 [00:00<00:01,  2.15it/s][I 2023-07-17 02:00:43,201] Trial 60 finished with value: 0.44159755774159165 and parameters: {'min_child_samples': 100}. Best is trial 60 with value: 0.44159755774159165.\n",
      "min_data_in_leaf, val_score: 0.441598:  20%|##        | 1/5 [00:00<00:01,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.441598 + 0.00576418\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000777 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004125 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000707 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000726 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.441598:  40%|####      | 2/5 [00:01<00:01,  1.50it/s][I 2023-07-17 02:00:44,007] Trial 61 finished with value: 0.4445562989657347 and parameters: {'min_child_samples': 5}. Best is trial 60 with value: 0.44159755774159165.\n",
      "min_data_in_leaf, val_score: 0.441598:  40%|####      | 2/5 [00:01<00:01,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.441598 + 0.00576418\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000729 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000722 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.441598:  60%|######    | 3/5 [00:01<00:01,  1.59it/s][I 2023-07-17 02:00:44,594] Trial 62 finished with value: 0.4420233063032205 and parameters: {'min_child_samples': 50}. Best is trial 60 with value: 0.44159755774159165.\n",
      "min_data_in_leaf, val_score: 0.441598:  60%|######    | 3/5 [00:01<00:01,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.441598 + 0.00576418\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000708 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000771 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000744 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.441598:  80%|########  | 4/5 [00:02<00:00,  1.66it/s][I 2023-07-17 02:00:45,154] Trial 63 finished with value: 0.44278086776464676 and parameters: {'min_child_samples': 25}. Best is trial 60 with value: 0.44159755774159165.\n",
      "min_data_in_leaf, val_score: 0.441598:  80%|########  | 4/5 [00:02<00:00,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.441598 + 0.00576418\n",
      "[LightGBM] [Info] Number of positive: 4070, number of negative: 14290\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000687 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4093, number of negative: 14267\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4047, number of negative: 14313\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004950 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4027, number of negative: 14333\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] Number of positive: 4067, number of negative: 14293\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000700 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3265\n",
      "[LightGBM] [Info] Number of data points in the train set: 18360, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221678 -> initscore=-1.255917\n",
      "[LightGBM] [Info] Start training from score -1.255917\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.222930 -> initscore=-1.248671\n",
      "[LightGBM] [Info] Start training from score -1.248671\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.220425 -> initscore=-1.263192\n",
      "[LightGBM] [Info] Start training from score -1.263192\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.219336 -> initscore=-1.269543\n",
      "[LightGBM] [Info] Start training from score -1.269543\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221514 -> initscore=-1.256864\n",
      "[LightGBM] [Info] Start training from score -1.256864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.441598: 100%|##########| 5/5 [00:02<00:00,  1.74it/s][I 2023-07-17 02:00:45,680] Trial 64 finished with value: 0.4442027456301566 and parameters: {'min_child_samples': 10}. Best is trial 60 with value: 0.44159755774159165.\n",
      "min_data_in_leaf, val_score: 0.441598: 100%|##########| 5/5 [00:02<00:00,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[10]\tcv_agg's binary_logloss: 0.441598 + 0.00576418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# define hyperparameters to be tuned\n",
    "params = {\"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",\n",
    "        # \"verbosity\": 1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"n_jobs\": -1\n",
    "}\n",
    "\n",
    "dtrain = lgb_optuna.Dataset(X_train, y_train)\n",
    "dval = lgb_optuna.Dataset(X_val, y_val)\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=8)\n",
    "tuner = lgb_optuna.LightGBMTunerCV(params=params,train_set=dtrain,folds=folds,num_boost_round=10,\n",
    "                                    seed=8,optuna_seed=8,time_budget=60,\n",
    "                                    callbacks=[lgb.early_stopping(3),lgb.log_evaluation(100)])\n",
    "tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary',\n",
       " 'metric': 'binary_logloss',\n",
       " 'boosting_type': 'gbdt',\n",
       " 'n_jobs': -1,\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 1.4299183901170552e-06,\n",
       " 'lambda_l2': 3.3642476428392784e-07,\n",
       " 'num_leaves': 164,\n",
       " 'feature_fraction': 0.9840000000000001,\n",
       " 'bagging_fraction': 1.0,\n",
       " 'bagging_freq': 0,\n",
       " 'min_child_samples': 100}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom LGBM+Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_metric(train,valid):\n",
    "    alpha = 3\n",
    "    beta = 0.1\n",
    "    return alpha*valid - beta*abs(train-valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # define hyperparameters to be tuned\n",
    "    params = {\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 100),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 0.1),\n",
    "        \"n_estimators\": 1000,\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"subsample\": trial.suggest_uniform(\"subsample\", 0.1, 1),\n",
    "        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 1),\n",
    "        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-5, 10),\n",
    "        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-5, 10),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "\n",
    "    }\n",
    "\n",
    "    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"auc\")\n",
    "\n",
    "    # define the model\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "    # train the model\n",
    "    # model.fit(X_train, y_train,\n",
    "    #            callbacks=[lgb.early_stopping(10),lgb.log_evaluation(100),pruning_callback])\n",
    "    cross_results = cross_validate(model, X_train, y_train, cv=5, scoring=\"roc_auc\", n_jobs=-1,\n",
    "                                   return_train_score=True,\n",
    "                                   fit_params={\"eval_set\":[(X_train,y_train),(X_val, y_val)],\n",
    "                                               \"eval_metric\":\"auc\",\n",
    "                                               \"callbacks\": [lgb.early_stopping(10),lgb.log_evaluation(100),pruning_callback]})\n",
    "\n",
    "    # evaluate the model\n",
    "    result = custom_metric(cross_results[\"train_score\"].mean(), cross_results[\"test_score\"].mean())\n",
    "\n",
    "    # return the objective value to be maximized\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-17 02:43:42,346] A new study created in memory with name: no-name-336bfa47-eafe-447f-ab17-48302c284c09\n",
      "/tmp/ipykernel_34418/1012821685.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 0.1),\n",
      "/tmp/ipykernel_34418/1012821685.py:8: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"subsample\": trial.suggest_uniform(\"subsample\", 0.1, 1),\n",
      "/tmp/ipykernel_34418/1012821685.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 1),\n",
      "/tmp/ipykernel_34418/1012821685.py:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-5, 10),\n",
      "/tmp/ipykernel_34418/1012821685.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-5, 10),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's auc: 0.808465\tvalid_0's binary_logloss: 0.51729\tvalid_1's auc: 0.786613\tvalid_1's binary_logloss: 0.518026\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid_0's auc: 0.810995\tvalid_0's binary_logloss: 0.519705\tvalid_1's auc: 0.786687\tvalid_1's binary_logloss: 0.520352\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalid_0's auc: 0.815413\tvalid_0's binary_logloss: 0.482269\tvalid_1's auc: 0.782822\tvalid_1's binary_logloss: 0.486261\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid_0's auc: 0.814871\tvalid_0's binary_logloss: 0.48435\tvalid_1's auc: 0.784532\tvalid_1's binary_logloss: 0.488695\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid_0's auc: 0.815908\tvalid_0's binary_logloss: 0.4853\tvalid_1's auc: 0.783302\tvalid_1's binary_logloss: 0.489371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-17 02:43:44,297] Trial 0 finished with value: 2.334146727384563 and parameters: {'num_leaves': 76, 'learning_rate': 0.006517726760599603, 'min_child_samples': 71, 'subsample': 0.34500310254184346, 'colsample_bytree': 0.8459297633481696, 'reg_alpha': 0.21108797152737233, 'reg_lambda': 0.0005712424191967936}. Best is trial 0 with value: 2.334146727384563.\n",
      "/tmp/ipykernel_34418/1012821685.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 0.1),\n",
      "/tmp/ipykernel_34418/1012821685.py:8: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"subsample\": trial.suggest_uniform(\"subsample\", 0.1, 1),\n",
      "/tmp/ipykernel_34418/1012821685.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 1),\n",
      "/tmp/ipykernel_34418/1012821685.py:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-5, 10),\n",
      "/tmp/ipykernel_34418/1012821685.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-5, 10),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's auc: 0.798654\tvalid_0's binary_logloss: 0.512658\tvalid_1's auc: 0.780598\tvalid_1's binary_logloss: 0.513796\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[21]\tvalid_0's auc: 0.803404\tvalid_0's binary_logloss: 0.492165\tvalid_1's auc: 0.780416\tvalid_1's binary_logloss: 0.495\n",
      "Early stopping, best iteration is:\n",
      "[50]\tvalid_0's auc: 0.808691\tvalid_0's binary_logloss: 0.460697\tvalid_1's auc: 0.783401\tvalid_1's binary_logloss: 0.467858\n",
      "Early stopping, best iteration is:\n",
      "[50]\tvalid_0's auc: 0.809104\tvalid_0's binary_logloss: 0.461137\tvalid_1's auc: 0.783192\tvalid_1's binary_logloss: 0.46735\n",
      "Early stopping, best iteration is:\n",
      "[21]\tvalid_0's auc: 0.804389\tvalid_0's binary_logloss: 0.4929\tvalid_1's auc: 0.782474\tvalid_1's binary_logloss: 0.495718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-17 02:43:45,621] Trial 1 finished with value: 2.3355376215412553 and parameters: {'num_leaves': 52, 'learning_rate': 0.010922604884572877, 'min_child_samples': 16, 'subsample': 0.21720552601343146, 'colsample_bytree': 0.5141748576189217, 'reg_alpha': 0.0010408459185322007, 'reg_lambda': 0.004639817684315513}. Best is trial 1 with value: 2.3355376215412553.\n",
      "/tmp/ipykernel_34418/1012821685.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 0.1),\n",
      "/tmp/ipykernel_34418/1012821685.py:8: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"subsample\": trial.suggest_uniform(\"subsample\", 0.1, 1),\n",
      "/tmp/ipykernel_34418/1012821685.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 1),\n",
      "/tmp/ipykernel_34418/1012821685.py:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-5, 10),\n",
      "/tmp/ipykernel_34418/1012821685.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-5, 10),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[31]\tvalid_0's auc: 0.777132\tvalid_0's binary_logloss: 0.489679\tvalid_1's auc: 0.770135\tvalid_1's binary_logloss: 0.489732\n",
      "Early stopping, best iteration is:\n",
      "[31]\tvalid_0's auc: 0.776961\tvalid_0's binary_logloss: 0.489661\tvalid_1's auc: 0.769667\tvalid_1's binary_logloss: 0.489664\n",
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's auc: 0.777019\tvalid_0's binary_logloss: 0.488968\tvalid_1's auc: 0.770637\tvalid_1's binary_logloss: 0.489064\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid_0's auc: 0.777158\tvalid_0's binary_logloss: 0.480399\tvalid_1's auc: 0.770781\tvalid_1's binary_logloss: 0.480587\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's auc: 0.775391\tvalid_0's binary_logloss: 0.515723\tvalid_1's auc: 0.770966\tvalid_1's binary_logloss: 0.515588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-17 02:43:46,643] Trial 2 finished with value: 2.317499314779014 and parameters: {'num_leaves': 8, 'learning_rate': 0.01058571189018737, 'min_child_samples': 88, 'subsample': 0.6644746509023516, 'colsample_bytree': 0.4411858394302519, 'reg_alpha': 0.351506601269175, 'reg_lambda': 0.002450593752759703}. Best is trial 1 with value: 2.3355376215412553.\n",
      "/tmp/ipykernel_34418/1012821685.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 0.1),\n",
      "/tmp/ipykernel_34418/1012821685.py:8: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"subsample\": trial.suggest_uniform(\"subsample\", 0.1, 1),\n",
      "/tmp/ipykernel_34418/1012821685.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 1),\n",
      "/tmp/ipykernel_34418/1012821685.py:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-5, 10),\n",
      "/tmp/ipykernel_34418/1012821685.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-5, 10),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's auc: 0.792051\tvalid_0's binary_logloss: 0.513237\tvalid_1's auc: 0.77997\tvalid_1's binary_logloss: 0.513782\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's auc: 0.793524\tvalid_0's binary_logloss: 0.486925\tvalid_1's auc: 0.781441\tvalid_1's binary_logloss: 0.488656\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's auc: 0.795209\tvalid_0's binary_logloss: 0.481448\tvalid_1's auc: 0.779993\tvalid_1's binary_logloss: 0.48353\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid_0's auc: 0.796989\tvalid_0's binary_logloss: 0.461445\tvalid_1's auc: 0.781929\tvalid_1's binary_logloss: 0.464525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-17 02:43:47,983] Trial 3 finished with value: 2.334090958954102 and parameters: {'num_leaves': 42, 'learning_rate': 0.012788350748562717, 'min_child_samples': 75, 'subsample': 0.47704841661253106, 'colsample_bytree': 0.5822469476383256, 'reg_alpha': 5.798711854425655, 'reg_lambda': 0.7500324329778782}. Best is trial 1 with value: 2.3355376215412553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[44]\tvalid_0's auc: 0.798396\tvalid_0's binary_logloss: 0.466538\tvalid_1's auc: 0.781817\tvalid_1's binary_logloss: 0.469764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34418/1012821685.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 0.1),\n",
      "/tmp/ipykernel_34418/1012821685.py:8: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"subsample\": trial.suggest_uniform(\"subsample\", 0.1, 1),\n",
      "/tmp/ipykernel_34418/1012821685.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 1),\n",
      "/tmp/ipykernel_34418/1012821685.py:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-5, 10),\n",
      "/tmp/ipykernel_34418/1012821685.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-5, 10),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's auc: 0.812215\tvalid_0's binary_logloss: 0.46547\tvalid_1's auc: 0.779071\tvalid_1's binary_logloss: 0.472663\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's auc: 0.820024\tvalid_0's binary_logloss: 0.437244\tvalid_1's auc: 0.783015\tvalid_1's binary_logloss: 0.449066\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid_0's auc: 0.837643\tvalid_0's binary_logloss: 0.402947\tvalid_1's auc: 0.781253\tvalid_1's binary_logloss: 0.431319\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid_0's auc: 0.848087\tvalid_0's binary_logloss: 0.391309\tvalid_1's auc: 0.78071\tvalid_1's binary_logloss: 0.428793\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's auc: 0.869952\tvalid_0's binary_logloss: 0.370407\tvalid_1's auc: 0.783243\tvalid_1's binary_logloss: 0.426811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-17 02:43:49,407] Trial 4 finished with value: 2.323643963436013 and parameters: {'num_leaves': 82, 'learning_rate': 0.09377150408977246, 'min_child_samples': 75, 'subsample': 0.8053931747179419, 'colsample_bytree': 0.8685092682761308, 'reg_alpha': 0.0003640233409419699, 'reg_lambda': 1.8792051061442552e-05}. Best is trial 1 with value: 2.3355376215412553.\n",
      "/tmp/ipykernel_34418/1012821685.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 0.1),\n",
      "/tmp/ipykernel_34418/1012821685.py:8: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"subsample\": trial.suggest_uniform(\"subsample\", 0.1, 1),\n",
      "/tmp/ipykernel_34418/1012821685.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 1),\n",
      "/tmp/ipykernel_34418/1012821685.py:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-5, 10),\n",
      "/tmp/ipykernel_34418/1012821685.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-5, 10),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's auc: 0.797032\tvalid_0's binary_logloss: 0.526723\tvalid_1's auc: 0.776849\tvalid_1's binary_logloss: 0.526867\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's auc: 0.797057\tvalid_0's binary_logloss: 0.526724\tvalid_1's auc: 0.778906\tvalid_1's binary_logloss: 0.526865\n",
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's auc: 0.79892\tvalid_0's binary_logloss: 0.526562\tvalid_1's auc: 0.776112\tvalid_1's binary_logloss: 0.52671\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's auc: 0.798639\tvalid_0's binary_logloss: 0.526725\tvalid_1's auc: 0.778271\tvalid_1's binary_logloss: 0.526866\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-17 02:43:50,234] Trial 5 finished with value: 2.324176422805145 and parameters: {'num_leaves': 51, 'learning_rate': 0.0011167627916494577, 'min_child_samples': 42, 'subsample': 0.5130779393066046, 'colsample_bytree': 0.3180995191842684, 'reg_alpha': 3.7307057951249506e-05, 'reg_lambda': 2.35740771986743}. Best is trial 1 with value: 2.3355376215412553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[9]\tvalid_0's auc: 0.79885\tvalid_0's binary_logloss: 0.526773\tvalid_1's auc: 0.775625\tvalid_1's binary_logloss: 0.526919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34418/1012821685.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 0.1),\n",
      "/tmp/ipykernel_34418/1012821685.py:8: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"subsample\": trial.suggest_uniform(\"subsample\", 0.1, 1),\n",
      "/tmp/ipykernel_34418/1012821685.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 1),\n",
      "/tmp/ipykernel_34418/1012821685.py:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-5, 10),\n",
      "/tmp/ipykernel_34418/1012821685.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-5, 10),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's auc: 0.796381\tvalid_0's binary_logloss: 0.525137\tvalid_1's auc: 0.782343\tvalid_1's binary_logloss: 0.525288\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's auc: 0.799649\tvalid_0's binary_logloss: 0.525465\tvalid_1's auc: 0.779953\tvalid_1's binary_logloss: 0.525648\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's auc: 0.795701\tvalid_0's binary_logloss: 0.525133\tvalid_1's auc: 0.777968\tvalid_1's binary_logloss: 0.525301\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's auc: 0.797727\tvalid_0's binary_logloss: 0.525128\tvalid_1's auc: 0.783082\tvalid_1's binary_logloss: 0.52528\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-17 02:43:51,305] Trial 6 finished with value: 2.3267645550674154 and parameters: {'num_leaves': 55, 'learning_rate': 0.0025290189700067057, 'min_child_samples': 99, 'subsample': 0.6886610014426929, 'colsample_bytree': 0.4250746229225729, 'reg_alpha': 0.0002872077452054378, 'reg_lambda': 8.77466815544392}. Best is trial 1 with value: 2.3355376215412553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[31]\tvalid_0's auc: 0.801086\tvalid_0's binary_logloss: 0.516325\tvalid_1's auc: 0.782523\tvalid_1's binary_logloss: 0.516826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34418/1012821685.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 0.1),\n",
      "/tmp/ipykernel_34418/1012821685.py:8: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"subsample\": trial.suggest_uniform(\"subsample\", 0.1, 1),\n",
      "/tmp/ipykernel_34418/1012821685.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 1),\n",
      "/tmp/ipykernel_34418/1012821685.py:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-5, 10),\n",
      "/tmp/ipykernel_34418/1012821685.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-5, 10),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's auc: 0.770803\tvalid_0's binary_logloss: 0.52557\tvalid_1's auc: 0.759427\tvalid_1's binary_logloss: 0.525714\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's auc: 0.773348\tvalid_0's binary_logloss: 0.525604\tvalid_1's auc: 0.758216\tvalid_1's binary_logloss: 0.525733\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's auc: 0.77285\tvalid_0's binary_logloss: 0.525573\tvalid_1's auc: 0.757456\tvalid_1's binary_logloss: 0.525727\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's auc: 0.772474\tvalid_0's binary_logloss: 0.525561\tvalid_1's auc: 0.764264\tvalid_1's binary_logloss: 0.52569\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's auc: 0.774118\tvalid_0's binary_logloss: 0.525639\tvalid_1's auc: 0.76236\tvalid_1's binary_logloss: 0.525764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-17 02:43:51,947] Trial 7 finished with value: 2.2578721550018086 and parameters: {'num_leaves': 46, 'learning_rate': 0.006933789716021928, 'min_child_samples': 46, 'subsample': 0.7389029233310372, 'colsample_bytree': 0.11987098176641754, 'reg_alpha': 0.11032982657763504, 'reg_lambda': 0.03803226117604845}. Best is trial 1 with value: 2.3355376215412553.\n",
      "/tmp/ipykernel_34418/1012821685.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 0.1),\n",
      "/tmp/ipykernel_34418/1012821685.py:8: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"subsample\": trial.suggest_uniform(\"subsample\", 0.1, 1),\n",
      "/tmp/ipykernel_34418/1012821685.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 1),\n",
      "/tmp/ipykernel_34418/1012821685.py:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-5, 10),\n",
      "/tmp/ipykernel_34418/1012821685.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-5, 10),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's auc: 0.815396\tvalid_0's binary_logloss: 0.518998\tvalid_1's auc: 0.784233\tvalid_1's binary_logloss: 0.519967\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's auc: 0.816041\tvalid_0's binary_logloss: 0.513962\tvalid_1's auc: 0.783888\tvalid_1's binary_logloss: 0.51558\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's auc: 0.822777\tvalid_0's binary_logloss: 0.480979\tvalid_1's auc: 0.782812\tvalid_1's binary_logloss: 0.486425\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's auc: 0.822496\tvalid_0's binary_logloss: 0.478632\tvalid_1's auc: 0.784421\tvalid_1's binary_logloss: 0.484172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-17 02:43:53,470] Trial 8 finished with value: 2.3325438715797966 and parameters: {'num_leaves': 89, 'learning_rate': 0.008423247483704092, 'min_child_samples': 8, 'subsample': 0.9747751715882267, 'colsample_bytree': 0.7999342455855488, 'reg_alpha': 0.18099973462968197, 'reg_lambda': 1.3476104897239582}. Best is trial 1 with value: 2.3355376215412553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's auc: 0.823444\tvalid_0's binary_logloss: 0.479283\tvalid_1's auc: 0.782176\tvalid_1's binary_logloss: 0.485626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34418/1012821685.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 0.1),\n",
      "/tmp/ipykernel_34418/1012821685.py:8: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"subsample\": trial.suggest_uniform(\"subsample\", 0.1, 1),\n",
      "/tmp/ipykernel_34418/1012821685.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 1),\n",
      "/tmp/ipykernel_34418/1012821685.py:10: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-5, 10),\n",
      "/tmp/ipykernel_34418/1012821685.py:11: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-5, 10),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid_0's auc: 0.812465\tvalid_0's binary_logloss: 0.489545\tvalid_1's auc: 0.77603\tvalid_1's binary_logloss: 0.494235\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's auc: 0.830249\tvalid_0's binary_logloss: 0.443747\tvalid_1's auc: 0.780924\tvalid_1's binary_logloss: 0.458235\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's auc: 0.831536\tvalid_0's binary_logloss: 0.444461\tvalid_1's auc: 0.779195\tvalid_1's binary_logloss: 0.459714\n",
      "Early stopping, best iteration is:\n",
      "[87]\tvalid_0's auc: 0.855598\tvalid_0's binary_logloss: 0.396255\tvalid_1's auc: 0.785556\tvalid_1's binary_logloss: 0.432262\n",
      "[100]\tvalid_0's auc: 0.860892\tvalid_0's binary_logloss: 0.389069\tvalid_1's auc: 0.784053\tvalid_1's binary_logloss: 0.431338\n",
      "Early stopping, best iteration is:\n",
      "[93]\tvalid_0's auc: 0.857147\tvalid_0's binary_logloss: 0.393017\tvalid_1's auc: 0.785176\tvalid_1's binary_logloss: 0.431791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-17 02:43:55,260] Trial 9 finished with value: 2.321250103211864 and parameters: {'num_leaves': 96, 'learning_rate': 0.03137815008200072, 'min_child_samples': 44, 'subsample': 0.583273509001468, 'colsample_bytree': 0.20807940169870992, 'reg_alpha': 0.00018003298352337068, 'reg_lambda': 2.0885911295593627e-05}. Best is trial 1 with value: 2.3355376215412553.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\",\n",
    "                            pruner=optuna.pruners.MedianPruner(n_warmup_steps=10))\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_leaves': 52,\n",
       " 'learning_rate': 0.010922604884572877,\n",
       " 'min_child_samples': 16,\n",
       " 'subsample': 0.21720552601343146,\n",
       " 'colsample_bytree': 0.5141748576189217,\n",
       " 'reg_alpha': 0.0010408459185322007,\n",
       " 'reg_lambda': 0.004639817684315513}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3355376215412553"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
